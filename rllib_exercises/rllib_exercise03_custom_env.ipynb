{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 3 - Custom Environments and Reward Shaping\n",
    "\n",
    "**GOAL:** The goal of this exercise is to demonstrate how to adapt your own problem to use RLlib.\n",
    "\n",
    "To understand how to use **RLlib**, see the documentation at http://rllib.io.\n",
    "\n",
    "RLlib is not only easy to use in simulated benchmarks but also in the real-world. Here, we will cover two important concepts: how to create your own Markov Decision Process abstraction, and how to shape the reward of your environment so make your agent more effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:45:45,544\tWARNING worker.py:1340 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-06-16 19:45:45,546\tERROR worker.py:1346 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import ray\n",
    "import numpy as np\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "import test_exercises\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do when formulating an RL problem is to specify the dimensions of your observation space and action space. Abstractions for these are provided in ``gym``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  10\n",
      "Random sample of this space:  [0, 5, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "num_states = 10\n",
    "discrete = spaces.Discrete(num_states)\n",
    "\n",
    "print(\"Number of states: \", discrete.n)\n",
    "print(\"Random sample of this space: \", [discrete.sample() for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1:** Match different actions to their corresponding space.\n",
    "\n",
    "The purpose of this exercise is to familiarize you with \n",
    "\n",
    "\n",
    "Use `help(spaces)` or `help([specific space])` (i.e., `help(spaces.Discrete)`) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "action_space_map = {\n",
    "    \"discrete_10\": spaces.Discrete(10),\n",
    "    \"box_1\": spaces.Box(0, 1, shape=(1,)),\n",
    "    \"box_3x1\": spaces.Box(-2, 2, shape=(3, 1)),\n",
    "    \"discrete_10\": spaces.Discrete(10),\n",
    "    \"multi_discrete\": spaces.MultiDiscrete([ 5, 2, 2, 4 ])\n",
    "}\n",
    "\n",
    "action_space_jumble = {\n",
    "    \"discrete_10\": 1,\n",
    "    \"box_3x1\": np.array([[-1.2657754], [-1.6528835], [ 0.5982418]]),\n",
    "    \"box_1\": np.array([0.89089584]),\n",
    "    \"multi_discrete\": np.array([0, 0, 0, 2])\n",
    "}\n",
    "\n",
    "\n",
    "for space_id, state in action_space_jumble.items():\n",
    "    assert action_space_map[space_id].contains(state), (\n",
    "        \"Looks like {} to {} is matched incorrectly.\".format(space_id, state))\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2**: Setting up a custom environment with rewards\n",
    "\n",
    "We'll setup an `n-Chain` environment. \n",
    "\n",
    "This environment presents moves along a linear chain of states, with two actions:\n",
    "\n",
    "     (0) forward, which moves along the chain but returns no reward\n",
    "     (1) backward, which returns to the beginning and has a small reward\n",
    "\n",
    "The end of the chain, however, presents a large reward, and by moving 'forward', at the end of the chain this large reward can be repeated.\n",
    "\n",
    "#### Step 1: Implement ``ChainEnv._setup_spaces``\n",
    "\n",
    "We'll use a `spaces.Discrete` action space and observation space. Implement `ChainEnv._setup_spaces` so that `self.action_space` and `self.obseration_space` are proper gym spaces.\n",
    "  \n",
    "1. Observation space corresponds to the current state in the chain and is an integer in ``[0 to n-1]``.\n",
    "2. Action space corresponds to the two actions and is an integer in ``[0, 1]``.\n",
    "\n",
    "You should see a message indicating tests passing when done correctly. \n",
    "\n",
    "#### Step 2: Implement a reward function.\n",
    "\n",
    "When `env.step` is called, it returns a tuple of ``(state, reward, done, info)``. Right now, the reward is always 0. \n",
    "\n",
    "Implement it so that \n",
    "\n",
    "1. ``action == 1`` will return `self.small_reward`. This corresponds to the `backward` action, which provides a small reward but returns the agent to the beginning\n",
    "2. ``action == 0`` will return 0 if `self.state < self.n - 1`. This corresponds to the `backward` action, which provides a small reward but returns the agent to the beginning\n",
    "3. ``action == 0`` will return `self.large_reward` if `self.state == self.n - 1`.\n",
    "\n",
    "You should see a message indicating tests passing when done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if spaces have been setup correctly...\n",
      "Success! You've setup the spaces correctly.\n",
      "Testing if reward has been setup correctly...\n",
      "Success! You've setup the rewards correctly.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "class ChainEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, env_config = None):\n",
    "        env_config = env_config or {}\n",
    "        self.n = env_config.get(\"n\", 50)\n",
    "        self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n",
    "        self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self._horizon = 200\n",
    "        self._counter = 0  # For terminating the episode\n",
    "        self._setup_spaces()\n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        # TODO: Implement this so that it passes tests\n",
    "#         self.action_space = None\n",
    "#         self.observation_space = None\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Discrete(self.n)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning, get small reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "#             reward = -1 \n",
    "            reward = self.small_reward\n",
    "            ##############\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "#             reward = -1 \n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "#             reward = -1\n",
    "            reward = self.large_reward\n",
    "            ##############\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self._counter = 0\n",
    "        return self.state\n",
    "    \n",
    "test_exercises.test_chain_env_spaces(ChainEnv)\n",
    "test_exercises.test_chain_env_reward(ChainEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now train a policy on the environment and evaluate this policy on our environment.\n",
    "\n",
    "You'll see that despite an extremely high reward, the policy has barely explored the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:45:56,188\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "2019-06-16 19:45:56,202\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/rllib/models/fcnet.py:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/rliaw/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/rliaw/miniconda3/lib/python3.7/site-packages/ray/rllib/models/action_dist.py:123: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:45:56,468\tINFO dynamic_tf_policy.py:313 -- Initializing loss function with dummy input:\n",
      "\n",
      "{ 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "  'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "  'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "  'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "  'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "  'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 100) dtype=float32>,\n",
      "  'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 100) dtype=float32>,\n",
      "  'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "  'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "  'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "  'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "  'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rliaw/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/rliaw/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:45:57,395\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x10b057b00>}\n",
      "2019-06-16 19:45:57,396\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.OneHotPreprocessor object at 0x14ce1ceb8>}\n",
      "2019-06-16 19:45:57,397\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x10b0571d0>}\n",
      "2019-06-16 19:45:57,431\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "2019-06-16 19:46:09,077\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'inputs': [ np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.496),\n",
      "              np.ndarray((4000,), dtype=float32, min=0.0, max=2.0, mean=0.993),\n",
      "              np.ndarray((4000, 100), dtype=float32, min=0.0, max=1.0, mean=0.01),\n",
      "              np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "              np.ndarray((4000,), dtype=float32, min=-2.24, max=1.894, mean=-0.0),\n",
      "              np.ndarray((4000, 2), dtype=float32, min=-0.002, max=0.002, mean=0.001),\n",
      "              np.ndarray((4000,), dtype=float32, min=0.0, max=105.36, mean=57.099),\n",
      "              np.ndarray((4000,), dtype=float32, min=-0.001, max=0.002, mean=0.0)],\n",
      "  'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "                    <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "                    <tf.Tensor 'default_policy/observation:0' shape=(?, 100) dtype=float32>,\n",
      "                    <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "                    <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "                    <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "                    <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "                    <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "  'state_inputs': []}\n",
      "\n",
      "2019-06-16 19:46:09,079\tINFO multi_gpu_impl.py:191 -- Divided 4000 rollout sequences, each of length 1, among 1 devices.\n"
     ]
    }
   ],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "\n",
    "trainer = PPOTrainer(trainer_config, ChainEnv);\n",
    "for i in range(5):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward you've received is: 336. Congratulations!\n",
      "Max state you've visited is: 4. This is out of 100 states.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.models import ModelCatalog\n",
    "env = ChainEnv({})\n",
    "prepare = ModelCatalog.get_preprocessor(env)\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(\"Cumulative reward you've received is: {}. Congratulations!\".format(cumulative_reward))\n",
    "print(\"Max state you've visited is: {}. This is out of {} states.\".format(max_state, env.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Shaping the reward to encourage proper behavior.\n",
    "\n",
    "You'll see that despite an extremely high reward, the policy has barely explored the state space. This is often the situation - where the reward designed to encourage a particular solution is suboptimal, and the behavior created is unintended.\n",
    "\n",
    "#### Modify `ShapedChainEnv.step` to provide a reward that encourages the policy to traverse the chain (not just stick to 0). Do not change the behavior of the environment (the action -> state behavior should be the same).\n",
    "\n",
    "You can change the reward to be whatever you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing if behavior has been changed...\n",
      "Success! Behavior of environment is correct.\n"
     ]
    }
   ],
   "source": [
    "class ShapedChainEnv(ChainEnv):\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning\n",
    "            reward = -1 \n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = self.state / self.n\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain\n",
    "            reward = self.state / self.n\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "test_exercises.test_chain_env_behavior(ShapedChainEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `ShapedChainEnv` by running the cell below.\n",
    "\n",
    "This trains PPO on the new env and counts the number of states seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-16 19:49:56,630\tWARNING ppo.py:151 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "2019-06-16 19:49:56,645\tINFO rollout_worker.py:301 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "2019-06-16 19:49:58,020\tINFO rollout_worker.py:719 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x2823fd208>}\n",
      "2019-06-16 19:49:58,021\tINFO rollout_worker.py:720 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.OneHotPreprocessor object at 0x2834cb588>}\n",
      "2019-06-16 19:49:58,022\tINFO rollout_worker.py:333 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x2834cb668>}\n",
      "2019-06-16 19:49:58,063\tINFO multi_gpu_optimizer.py:79 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward you've received is: -41.129999999999995!\n",
      "Max state you've visited is: 13. This is out of 100 states.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "This policy did not traverse many states.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-29751f040d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cumulative reward you've received is: {}!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Max state you've visited is: {}. This is out of {} states.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This policy did not traverse many states.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: This policy did not traverse many states."
     ]
    }
   ],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 2\n",
    "\n",
    "trainer = PPOTrainer(trainer_config, ShapedChainEnv);\n",
    "for i in range(5):\n",
    "    trainer.train()\n",
    "\n",
    "    from ray.rllib.models import ModelCatalog\n",
    "env = ShapedChainEnv({})\n",
    "prepare = ModelCatalog.get_preprocessor(env)\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(\"Cumulative reward you've received is: {}!\".format(cumulative_reward))\n",
    "print(\"Max state you've visited is: {}. This is out of {} states.\".format(max_state, env.n))\n",
    "assert (env.n - max_state) < 5, \"This policy did not traverse many states.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
