{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tune.png\" alt=\"Tune Logo\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is a scalable framework for model training and hyperparameter search with a focus on deep learning and deep reinforcement learning.\n",
    "\n",
    "**Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune\n",
    "\n",
    "**Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "\n",
    "**Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "\n",
    "**Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Tuning hyperparameters is often the most expensive part of the machine learning workflow. Tune is built to address this, demonstrating an efficient and scalable solution for this pain point.\n",
    "\n",
    "\n",
    "## Outline\n",
    "This tutorial will walk you through the following process:\n",
    "\n",
    "1. Creating and training a model on a toy dataset (MNIST)\n",
    "2. Integrating Tune into your workflow\n",
    "3. Trying out advanced features - plugging in an efficient scheduler\n",
    "4. Validating your trained model\n",
    "5. (Optional) Try out a search algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "limit_threads(4)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Creating a model to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Convolutional Neural Network model. Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. The details of how a Convolutional Neural Network works are unimportant here, but you're welcome to read more about them here: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "<img src=\"cnn.png\" alt=\"MNIST Visualization\" width=\"800\"/>\n",
    "\n",
    "\n",
    "This convolutional neural network model will be used classify digits (from the MNIST dataset).\n",
    "\n",
    "<img src=\"mnist.png\" alt=\"MNIST Visualization\" width=\"400\"/>\n",
    "\n",
    "This is a fairly simple dataset, but it enables us to explore Tune's functionality in depth.\n",
    "We will use 60,000 images to train the network. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the digit the image represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll specify some arguments and some reasonable defaults for this model. These are the hyperparameters settings that we will later use to further optimize this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Keras MNIST Example')\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, help='SGD momentum')\n",
    "parser.add_argument('--kernel1', type=int, default=3, help='Size of first kernel')\n",
    "parser.add_argument('--kernel2', type=int, default=3, help='Size of second kernel')\n",
    "parser.add_argument('--poolsize', type=int, default=2, help='Size of Poolin')\n",
    "parser.add_argument('--dropout1', type=float, default=0.25, help='Size of first kernel')\n",
    "parser.add_argument('--hidden', type=int, default=4, help='Size of Hidden Layer')\n",
    "parser.add_argument('--dropout2', type=float, default=0.5, help='Size of first kernel')\n",
    "\n",
    "DEFAULT_ARGS = vars(parser.parse_known_args()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below function will create and return a Convolutional Neural Network. You don't need to modify this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(parameters):\n",
    "    config = DEFAULT_ARGS.copy()  # This is obtained via the global scope\n",
    "    config.update(parameters)\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(config[\"kernel1\"], config[\"kernel1\"]),\n",
    "                     activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (config[\"kernel2\"], config[\"kernel2\"]), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(config[\"poolsize\"], config[\"poolsize\"])))\n",
    "    model.add(Dropout(config[\"dropout1\"]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config[\"hidden\"], activation='relu'))\n",
    "    model.add(Dropout(config[\"dropout2\"]))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(\n",
    "                      lr=config[\"lr\"], momentum=config[\"momentum\"]),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Setup a basic model training script.\n",
    "\n",
    "The process of training the neural network model occurs as follows:\n",
    "\n",
    "1. Feed the training data to the modelâ€”in this example, the `batch_of_data` and `batch_of_labels` arrays.\n",
    "2. The model learns to associate images and labels.\n",
    "\n",
    "**Exercise**: Finish the TODOs below. Here are a few hints:\n",
    "\n",
    "1) `data_generator` is an iterator that returns (`batch_of_data`, `batch_of_labels`), like follows:\n",
    "\n",
    "```python\n",
    "for batch_of_data, batch_of_labels in data_generator:\n",
    "    do_something_interesting()\n",
    "```\n",
    "2) You can use `model.fit(batch_of_data, batch_of_labels)` to repeatedly improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(args):\n",
    "    \"\"\"Loads data, does one pass over the data, and saves the weights.\"\"\"\n",
    "    data_generator = load_data()\n",
    "    model = make_model(args)\n",
    "    for batch_of_data, batch_of_labels in data_generator:\n",
    "        model.fit(batch_of_data, batch_of_labels)\n",
    "    ## TODO: use the `data_generator` to iterate over the data and improve the model\n",
    "    model.save_weights(\"./weights.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this above training script to make sure things work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_model = train_mnist(DEFAULT_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now quickly try out this model to see if it works as expected. We'll load the model with our trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try to write a digit into the box below. This will automatically save your input in a variable `data` behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tip: don't expect it to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_data(data)\n",
    "print(\"This model predicted your input as\", first_model.predict(prepared_data).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've now set up a model that we can use Tune to optimize!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up Tune\n",
    "\n",
    "One thing we might want to do now is find better hyperparameters so that our model trains more quickly and possibly to a higher accuracy. Let's make some minor modifications to utilize Tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune uses Ray as a backend, so we will first import and initialize Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune will automate and distribute your hyperparameter search by scheduling a number of **trials** on a machine. Each trial runs a user-defined Python function with a sampled set of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Two steps to use Tune:\n",
    "\n",
    "Step 1) For the function you wish to tune, we need to change the signature to a specific format as shown below. Specifically: **pass in a ``reporter`` object to the below `train_mnist_tune` class**.\n",
    "\n",
    "```python\n",
    "def trainable(config, reporter):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        config (dict): Parameters provided from the search algorithm\n",
    "            or variant generation.\n",
    "        reporter (Reporter): Handle to report intermediate metrics to Tune.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Step 2) We want to keep track of performance as the model is training. Specifically: **get the `mean_accuracy` from Keras, and call the ``reporter`` to report the `mean_accuracy` for every batch**. \n",
    "\n",
    "You can get model accuracy from Keras with the following code:\n",
    "\n",
    "```python\n",
    "mean_accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "```\n",
    "\n",
    "\n",
    "Example of using the reporter:\n",
    "\n",
    "```python\n",
    "def train_func(config, reporter):  # add a reporter arg\n",
    "    # ...\n",
    "    for data, target in dataset:\n",
    "        model.fit(data, target)\n",
    "        # save model here\n",
    "        accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "        reporter(mean_accuracy=accuracy) # report metrics\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_tune(config, reporter): ### TODO: Change this function signature following step 1 #####\n",
    "    data_generator = load_data()\n",
    "    model = make_model(config)\n",
    "    for i, (x_batch, y_batch) in enumerate(data_generator):\n",
    "        model.fit(x_batch, y_batch, verbose=0)\n",
    "        if i % 3 == 0:\n",
    "            last_checkpoint = \"weights_tune_{}.h5\".format(i)\n",
    "            model.save_weights(last_checkpoint)\n",
    "        ### Don't change above ############### \n",
    "        mean_accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "        reporter(mean_accuracy=result[1], timesteps_total=i, checkpoint=last_checkpoint)\n",
    "        ### TODO: Use the reporter here to fill out intermediate metrics following step 2###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take 30 seconds or so to run if incorrectly written\n",
    "assert test_reporter(train_mnist_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you've done this correctly, you now have properly converted your function to be Tune-compatible!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Let's now try to search over some parameters. \n",
    "\n",
    "*NOTE: You can find the documentation for this section here: https://ray.readthedocs.io/en/latest/tune-usage.html#specifying-experiments*\n",
    "\n",
    "In this section, we'll use some basic Tune features for training - namely specifying a stopping criteria and a search space. \n",
    "\n",
    "Let's first create a Tune Experiment specification. The relevant documentation for the Experiment class is here:\n",
    "\n",
    "```python\n",
    "class ray.tune.Experiment(name, run, stop=None, config=None, ... ):\n",
    "    \"\"\"Tracks experiment specifications.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Name of experiment.\n",
    "        run (function|class|str): The algorithm or model to train.\n",
    "            This may refer to the name of a built-on algorithm\n",
    "            (e.g. RLLib's DQN or PPO), a user-defined trainable\n",
    "            function or class, or the string identifier of a\n",
    "            trainable function or class registered in the tune registry.\n",
    "        stop (dict): The stopping criteria. The keys may be any field in\n",
    "            the return result of 'train()', whichever is reached first.\n",
    "            Defaults to empty dict.\n",
    "        config (dict): Algorithm-specific configuration for Tune variant\n",
    "            generation (e.g. env, hyperparams). Defaults to empty dict.\n",
    "            Custom search algorithms may ignore this.\n",
    "        trial_resources (dict): Machine resources to allocate per trial,\n",
    "            e.g. ``{\"cpu\": 64, \"gpu\": 8}``. Note that GPUs will not be\n",
    "            assigned unless you specify them here. Defaults to 1 CPU and 0\n",
    "            GPUs in ``Trainable.default_resource_request()``.\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**: First, **set the stopping criteria to when `mean_accuracy` passes `0.95`**. For example, to specify that trials will be stopped whenever they report `arbitrary_metric` that is `>= 500`, do:\n",
    "\n",
    "```python\n",
    "stop={\"arbitrary_metric\": 500}\n",
    "```\n",
    "\n",
    "\n",
    "**Part 2**: We also want to designate a search space. We'll search over *learning rate*, which sets the step size of our model update, and *momentum*, which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "\n",
    "You can use `tune.grid_search` to specify an axis of a grid search. By default, Tune also supports sampling parameters from user-specified lambda functions, which can be used independently or in combination with grid search.  The following example shows grid search over a set of values combined with random sampling from a lambda functions, generating 3 different trials. \n",
    "\n",
    "```python\n",
    "configuration = tune.Experiment(\n",
    "    # ...\n",
    "    config={\n",
    "        \"arbitrary_parameter1\": lambda spec: np.random.uniform(0.1, 100),\n",
    "        \"arbitrary_parameter2\": tune.grid_search([16, 64, 256]),\n",
    "        # ...\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Specifically, \n",
    "1. randomly search for learning rate `\"lr\"` between 0.001 to 0.1,\n",
    "2. do a grid search over `\"momentum\"` for `[0.2, 0.4, 0.6]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = tune.Experiment(\n",
    "    \"experiment_name\",\n",
    "    run=train_mnist_tune,\n",
    "    trial_resources={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 0.95},  # TODO: Part 1\n",
    "    config={\"lr\": lambda spec: np.random.uniform(0.001, 0.1),\n",
    "            \"momentum\": tune.grid_search([0.2, 0.4, 0.6])}  # TODO: Part 2\n",
    ")\n",
    "\n",
    "assert configuration.spec.get(\"stop\", {}).get(\"mean_accuracy\") == 0.95\n",
    "assert \"grid_search\" in configuration.spec.get(\"config\", {}).get(\"momentum\", {})\n",
    "assert \"lr\" in configuration.spec.get(\"config\", {})\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run our experiment with a single line of code. \n",
    "\n",
    "*Note*: Be sure pay attention to the `acc` metric next to each running trial. That indicates the most recently reported mean accuracy for that trial. This should evaluate in less than a minute. The output will look something similar to:\n",
    "\n",
    "```\n",
    "== Status ==\n",
    "Using FIFO scheduling algorithm.\n",
    "Resources requested: 8/8 CPUs, 0/1 GPUs\n",
    "Result logdir: .../ray_results/experiment_name\n",
    "RUNNING trials:\n",
    " - train_mnist_tune_0_lr=0.085836,momentum=0.2:\tRUNNING [pid=44320], 4 s, 3 iter, 0.406 acc\n",
    " - train_mnist_tune_1_lr=0.062562,momentum=0.4:\tRUNNING [pid=44321], 3 s, 2 iter, 0.219 acc\n",
    " - train_mnist_tune_2_lr=0.099461,momentum=0.6:\tRUNNING [pid=44317], 3 s, 2 iter, 0.281 acc\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.stdout\n",
    "trials = tune.run_experiments(configuration, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect the result below to be about `0.6`, although your mileage may vary (and it's OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best result is\", get_best_result(trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've run your first Tune experiment!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try using a scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Tune schedules trials in serial order with the `FIFOScheduler` class. However, you can also specify a custom scheduling algorithm that can early stop trials or perturb parameters. \n",
    "\n",
    "Let's use a state of the art algorithm, `HyperBand`, to scale up and accelerate our training. Hyperband is an algorithm that focuses on speeding up random search through adaptive resource allocation and early-stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**: Let's scale up our search. \n",
    "\n",
    "1) Sample the search space 5 times. (https://ray.readthedocs.io/en/latest/tune-usage.html#sampling-multiple-times).\n",
    "\n",
    "2) Search over another hyperparameter: `\"hidden\"` from 16 to 512 which specifies the size of the last neural network layer.\n",
    "\n",
    "Here, use `np.random.randint`. \n",
    "```python\n",
    "numpy.random.randint(low, high=None, size=None, dtype='l')\n",
    "    \"\"\"Return random integers from low (inclusive) to high (exclusive).\"\"\"\n",
    "```\n",
    "\n",
    "An extended version of the `Experiment` documentation is shown below. Note that you should expect a total of 15 trials, due to the usage of `grid_search`.\n",
    "\n",
    "```python\n",
    "class ray.tune.Experiment(name, run, stop=None, config=None, ... ):\n",
    "    \"\"\"Tracks experiment specifications.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): Name of experiment.\n",
    "        run (function|class|str): The algorithm or model to train.\n",
    "            This may refer to the name of a built-on algorithm\n",
    "            (e.g. RLLib's DQN or PPO), a user-defined trainable\n",
    "            function or class, or the string identifier of a\n",
    "            trainable function or class registered in the tune registry.\n",
    "        stop (dict): The stopping criteria. The keys may be any field in\n",
    "            the return result of 'train()', whichever is reached first.\n",
    "            Defaults to empty dict.\n",
    "        config (dict): Algorithm-specific configuration for Tune variant\n",
    "            generation (e.g. env, hyperparams). Defaults to empty dict.\n",
    "            Custom search algorithms may ignore this.\n",
    "        trial_resources (dict): Machine resources to allocate per trial,\n",
    "            e.g. ``{\"cpu\": 64, \"gpu\": 8}``. Note that GPUs will not be\n",
    "            assigned unless you specify them here. Defaults to 1 CPU and 0\n",
    "            GPUs in ``Trainable.default_resource_request()``.\n",
    "        num_samples (int): Number of times to sample from the\n",
    "            hyperparameter space. Defaults to 1. If `grid_search` is\n",
    "            provided as an argument, the grid will be repeated\n",
    "            `num_samples` of times.\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration2 = tune.Experiment(\n",
    "    \"experiment2\",\n",
    "    run=train_mnist_tune,\n",
    "    num_samples=5, ## TODO: Change this to 5\n",
    "    trial_resources={\"cpu\": 4},\n",
    "    stop={\"mean_accuracy\": 0.95},\n",
    "    config={\n",
    "        \"lr\": lambda spec: np.random.uniform(0.001, 0.1),\n",
    "        \"momentum\": tune.grid_search([0.2, 0.4, 0.6]),\n",
    "        \"hidden\": lambda spec: np.random.randint(16, high=513), ## TODO: Sample uniformly from 16 to 512\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**: Create an Asynchronous HyperBand Scheduler (https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband). The documentation is shown below. \n",
    "\n",
    "Be sure to set the `time_attr` to `timesteps_total` and `reward_attr` to `mean_accuracy`.\n",
    "\n",
    "```python\n",
    "class AsyncHyperBandScheduler(FIFOScheduler):\n",
    "    \"\"\"This provides HyperBand functionality to your search.\n",
    "    \n",
    "    Hyperband is an algorithm that focuses on speeding up \n",
    "    random search through adaptive resource allocation and early-stopping.\n",
    "\n",
    "    See https://openreview.net/forum?id=S1Y7OOlRZ\n",
    "\n",
    "    Args:\n",
    "        time_attr (str): A training result attr to use for comparing time.\n",
    "            Note that you can pass in something non-temporal such as\n",
    "            `training_iteration` as a measure of progress, the only requirement\n",
    "            is that the attribute should increase monotonically.\n",
    "        reward_attr (str): The training result objective value attribute. As\n",
    "            with `time_attr`, this may refer to any objective value. Stopping\n",
    "            procedures will use this attribute.\n",
    "        ...\n",
    "        \n",
    "    Examples:\n",
    "        >>> hyperband = AsyncHyperBandScheduler(\n",
    "        >>>     time_attr='timesteps_total',\n",
    "        >>>     reward_attr='mean_accuracy')\n",
    "    \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "## TODO: Create an Asynchronous HyperBand Scheduler\n",
    "hyperband = AsyncHyperBandScheduler(\n",
    "    time_attr='timesteps_total',\n",
    "    reward_attr='mean_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous configuration, pass in the HyperBand scheduler to `run_experiments`.\n",
    "\n",
    "Recall that the `run_experiments` API is:\n",
    "```python\n",
    "def run_experiments(experiments=None,\n",
    "                    search_alg=None,\n",
    "                    scheduler=None,\n",
    "                    ...):\n",
    "    \"\"\"Runs and blocks until all trials finish.\n",
    "\n",
    "    Args:\n",
    "        experiments (Experiment | list | dict): Experiments to run. Will be\n",
    "            passed to `search_alg` via `add_configurations`.\n",
    "        search_alg (SearchAlgorithm): Search Algorithm. Defaults to\n",
    "            BasicVariantGenerator.\n",
    "        scheduler (TrialScheduler): Scheduler for executing\n",
    "            the experiment. Choose among FIFO (default), MedianStopping,\n",
    "            AsyncHyperBand, and HyperBand.\n",
    "        ...\n",
    "    \n",
    "    Returns:\n",
    "        List of Trial objects, holding data for each executed trial.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Call `run_experiments` with the correct arguments. This may take multiple minutes for the experiment to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you should be using `configuration2` instead of `configuration`.\n",
    "trials = tune.run_experiments(configuration2, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now, let's get the best model from your search process, and check its validation accuracy compared to the first model we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = get_best_model(make_model, trials, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best model...\")\n",
    "evaluate(best_model)\n",
    "print(\"First model...\")\n",
    "evaluate(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your model on some manual inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write a digit into the canvas below. This will automatically load your input into the variable `final_data`. (Due to randomness, your mileage may vary. If your model is bad, try increasing the number of samples with `load_data(num_samples=X)` in the above defined trainables, or try the Optional Search Algorithm section below).\n",
    "\n",
    "Typically, the model doesn't work quite well on 1 and 8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(open(\"input_final.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_input = prepare_data(final_data)\n",
    "best = best_model.predict(manual_input).argmax()\n",
    "first = first_model.predict(manual_input).argmax()\n",
    "\n",
    "print(\"Best model got {}, first model got {}\".format(best, first))\n",
    "plt.imshow(manual_input.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you're now a Tune expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please: fill out this form to provide feedback on this tutorial!\n",
    "\n",
    "https://goo.gl/forms/NVTFjUKFz4TH8kgK2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Try using a search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is an execution layer, so we can combine powerful optimizers such as HyperOpt (https://github.com/hyperopt/hyperopt) with state-of-the-art algorithms such as HyperBand without modifying any model training code.\n",
    "\n",
    "The documentation to doing this is here: https://ray.readthedocs.io/en/latest/tune-searchalg.html#hyperopt-search-tree-structured-parzen-estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "    \"hidden\": hp.choice(\"hidden\", np.arange(16, 256, dtype=int)),\n",
    "}\n",
    "\n",
    "## TODO: CREATE A HyperOptObject\n",
    "hyperopt_search = HyperOptSearch(space, reward_attr=\"mean_accuracy\")\n",
    "\n",
    "## TODO: Pass in the object to Tune.\n",
    "good_results = tune.run_experiments(\n",
    "    configuration2, search_alg=hyperopt_search, scheduler=hyperband, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to compare your best model here\n",
    "best_model2 = get_best_model(make_model, good_results, metric=\"mean_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the canvas data from above \n",
    "\n",
    "manual_input = prepare_data(final_data)\n",
    "best = best_model.predict(manual_input).argmax()\n",
    "best2 = best_model2.predict(manual_input).argmax()\n",
    "first = first_model.predict(manual_input).argmax()\n",
    "\n",
    "print(\"Best model got {}, HyperOpt model got {}, first model got {}\".format(best, best2, first))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
