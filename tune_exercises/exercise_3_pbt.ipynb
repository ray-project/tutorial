{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Population-based Training with Tune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial, we'll show you how to use Population-based Training with Tune. \n",
    "\n",
    "<img src=\"tune-pbt.png\" alt=\"Tune Logo\" width=\"600\"/>\n",
    "\n",
    "https://deepmind.com/blog/population-based-training-neural-networks\n",
    "\n",
    "PBT trains a group of models (or agents) in parallel. Periodically, poorly performing models clone the state of the top performers, and a random mutation is applied to their hyperparameters in the hopes of outperforming the current top models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to optimize this trainable's accuracy. The accuracy increases\n",
    "fastest at the optimal lr, which is a function of the current accuracy.\n",
    "\n",
    "The optimal lr schedule for this problem is the triangle wave as follows.\n",
    "Note that many lr schedules for real models also follow this shape:\n",
    "\n",
    "     best lr\n",
    "      ^\n",
    "      |    /\\\n",
    "      |   /  \\\n",
    "      |  /    \\\n",
    "      | /      \\\n",
    "      ------------> accuracy\n",
    "\n",
    "In this problem, using PBT with a population of 2-4 is sufficient to\n",
    "roughly approximate this lr schedule. Higher population sizes will yield\n",
    "faster convergence. \n",
    "\n",
    "Note that training will not converge without PBT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray.tune import Trainable, run\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBTBenchmarkExample(Trainable):\n",
    "    def _setup(self, config):\n",
    "        self.lr = config[\"lr\"]\n",
    "        \n",
    "        # penalize exceeding lr by more than this multiple\n",
    "        self.tolerance = config[\"tolerance\"]\n",
    "        self.score = 0.0  # end = 1000\n",
    "\n",
    "    def _train(self):\n",
    "        midpoint = 100  # lr starts decreasing after acc > midpoint\n",
    "        noise_level = 2  # add gaussian noise to the acc increase\n",
    "        # triangle wave:\n",
    "        #  - start at 0.001 @ t=0,\n",
    "        #  - peak at 0.01 @ t=midpoint,\n",
    "        #  - end at 0.001 @ t=midpoint * 2,\n",
    "        if self.score < midpoint:\n",
    "            optimal_lr = 0.01 * self.score / midpoint\n",
    "        else:\n",
    "            optimal_lr = 0.01 - 0.01 * (self.score - midpoint) / midpoint\n",
    "        optimal_lr = min(0.01, max(0.001, optimal_lr))\n",
    "\n",
    "        # compute accuracy increase\n",
    "        q_err = max(self.lr, optimal_lr) / min(self.lr, optimal_lr)\n",
    "        if q_err < self.tolerance:\n",
    "            self.score += (1.0 / q_err) * random.random()\n",
    "        elif self.lr > optimal_lr:\n",
    "            self.accuracy -= (q_err - self.tolerance) * random.random()\n",
    "        self.score += noise_level * np.random.normal()\n",
    "        self.score = max(0, self.score)\n",
    "\n",
    "        return {\n",
    "            \"mean_accuracy\": self.score,\n",
    "            \"cur_lr\": self.lr,\n",
    "            \"optimal_lr\": optimal_lr,  # for debugging\n",
    "            \"q_err\": q_err,  # for debugging\n",
    "            \"done\": self.accuracy > midpoint * 2,\n",
    "        }\n",
    "\n",
    "    def _save(self, checkpoint_dir):\n",
    "        return {\"accuracy\": self.accuracy}\n",
    "\n",
    "    def _restore(self, checkpoint):\n",
    "        self.accuracy = checkpoint[\"accuracy\"]\n",
    "\n",
    "    def reset_config(self, new_config):\n",
    "        self.lr = new_config[\"lr\"]\n",
    "        self.tolerance = new_config[\"tolerance\"]\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=20,\n",
    "    hyperparam_mutations={\n",
    "        # distribution for resampling\n",
    "        \"lr\": lambda: random.uniform(0.0001, 0.02),\n",
    "        # allow perturbations within this set of categorical values\n",
    "        \"tolerance\": [2, 3, 10],\n",
    "    }\n",
    ")\n",
    "\n",
    "tune.run(\n",
    "    PBTBenchmarkExample,\n",
    "    name=\"pbt_test\",\n",
    "    scheduler=pbt,\n",
    "    reuse_actors=True,\n",
    "    stop={\"training_iteration\": 2000},\n",
    "    num_samples=4,\n",
    "    config={\n",
    "        \"lr\": 0.0001,\n",
    "        # note: this parameter is perturbed but has no effect on\n",
    "        # the model training in this example\n",
    "        \"tolerance\": 3,\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
