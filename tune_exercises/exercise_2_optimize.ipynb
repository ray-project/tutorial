{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Accelerated Hyperparameter Tuning For PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial, we'll show you how to use state-of-the-art hyperparameter tuning with Tune and PyTorch.\n",
    "\n",
    "<img src=\"tune-arch-simple.png\" alt=\"Tune Logo\" width=\"600\"/>\n",
    "\n",
    "Specifically, we'll leverage ASHA and Bayesian Optimization (via HyperOpt) without modifying your underlying code.\n",
    "\n",
    "Tune is a scalable framework for model training and hyperparameter search with a focus on deep learning and deep reinforcement learning.\n",
    "\n",
    "* **Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune \n",
    "* **Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "* **Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "* **Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: PyTorch Boilerplate Code\n",
    "\n",
    "Run the below cells to see what you would do with Tune without any additional optimization techniques. You'll see that integrating Tune with PyTorch **requires 1 line of code**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is some basic imports. \n",
    "# Original Code here:\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from helper import train, test, ConvNet, get_data_loaders\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have some boiler plate code for a PyTorch training function. You can take a look at these functions in `helper.py`; there's no black magic happening. For example, `train` is simply a for loop over the data loader.\n",
    "\n",
    "```python\n",
    "    def train(model, optimizer, train_loader):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if batch_idx * len(data) > EPOCH_SIZE:\n",
    "                return\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "**TODO**: Add `track.log(mean_accuracy=acc)` within the training loop. `tune.track` allows Tune to keep track of current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config):\n",
    "    model = ConvNet(config)\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "    for i in range(20):\n",
    "        train(model, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "        # TODO: Add track.log(mean_accuracy=acc) here\n",
    "        if i % 5 == 0:\n",
    "            torch.save(model, \"./model.pth\") # This saves the model to the trial directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's run 1 trial, randomly sampling from a uniform distribution for learning rate and momentum. \n",
    "Run the below cell to run Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = dict(\n",
    "    name=\"train_mnist\",\n",
    "    stop={\"mean_accuracy\": 0.98},\n",
    "    return_trials=False\n",
    ")\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9)\n",
    "}\n",
    "\n",
    "# Note: use `ray.init(redis_address=...)` to enable distributed execution\n",
    "analysis = tune.run(train_mnist, config=search_space, **experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the performance of this trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.get_all_trial_dataframes()\n",
    "[d.mean_accuracy.plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Early Stopping with ASHA\n",
    "\n",
    "ASHA is a scalable algorithm for principled early stopping. How does it work? On a high level, it terminates trials that are less promising and allocates more time and resources to more promising trials. \n",
    "\n",
    "    The successive halving algorithm begins with all candidate configurations in the base rung and proceeds as follows:\n",
    "\n",
    "        1. Uniformly allocate a budget to a set of candidate hyperparameter configurations in a given rung.\n",
    "        2. Evaluate the performance of all candidate configurations.\n",
    "        3. Promote the top half of candidate configurations to the next rung.\n",
    "        4. Double the budget per configuration for the next rung and repeat until one configurations remains. \n",
    "        \n",
    "A textual representation:\n",
    "    \n",
    "           | Configurations | Epochs per \n",
    "           | Remaining      | Configuration\n",
    "    ---------------------------------------\n",
    "    Rung 1 | 27             | 1\n",
    "    Rung 2 | 9              | 3\n",
    "    Rung 3 | 3              | 9\n",
    "    Rung 4 | 1              | 27\n",
    "\n",
    "(from https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
    "\n",
    "Now, let's integrate this with a PyTorch codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Set up ASHA.\n",
    "\n",
    "1) Create an Asynchronous HyperBand Scheduler (ASHA).\n",
    "```python\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "custom_scheduler = ASHAScheduler(\n",
    "    reward_attr='mean_accuracy',\n",
    "    grace_period=1,\n",
    ")\n",
    "```\n",
    "\n",
    "*Note: Read the documentation on this step at https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband or call ``help(tune.schedulers.AsyncHyperBandScheduler)`` to learn more about the Asynchronous Hyperband Scheduler*\n",
    "\n",
    "2) With this, we can afford to **increase the search space by 5x**. To do this, set the parameter `num_samples`. For example,\n",
    "\n",
    "```python\n",
    "tune.run(... num_samples=30)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "custom_scheduler = \"FIX ME\"\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    num_samples=\"FIX ME\", \n",
    "    scheduler=custom_scheduler, \n",
    "    config=search_space, \n",
    "    **experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by wall-clock time\n",
    "\n",
    "dfs = analysis.get_all_trial_dataframes()\n",
    "# This plots everything on the same plot\n",
    "ax = None\n",
    "for d in dfs.values():\n",
    "    ax = d.plot(\"timestamp\", \"mean_accuracy\", ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by epoch\n",
    "ax = None\n",
    "for d in dfs.values():\n",
    "    ax = d.mean_accuracy.plot(ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Search Algorithms in Tune\n",
    "\n",
    "With Tune you can combine powerful Hyperparameter Search libraries such as HyperOpt (https://github.com/hyperopt/hyperopt) with state-of-the-art algorithms such as HyperBand without modifying any model training code. Tune allows you to use different search algorithms in combination with different trial schedulers. \n",
    "\n",
    "The documentation to doing this is here: https://ray.readthedocs.io/en/latest/tune-searchalg.html#hyperopt-search-tree-structured-parzen-estimators\n",
    "\n",
    "Currently, Tune offers the following search algorithms (and library integrations):\n",
    "\n",
    "* Grid Search and Random Search\n",
    "* BayesOpt\n",
    "* HyperOpt\n",
    "* SigOpt\n",
    "* Nevergrad\n",
    "* Scikit-Optimize\n",
    "* Ax\n",
    "\n",
    "Check out more at https://ray.readthedocs.io/en/latest/tune-searchalg.html\n",
    "\n",
    "**TODO:** Plug in `HyperOptSearch` into `tune.run` and enforce that only 2 trials can run concurrently, like this - \n",
    "\n",
    "```python\n",
    "    hyperopt_search = HyperOptSearch(space, max_concurrent=2, reward_attr=\"mean_accuracy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", 1e-10, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "}\n",
    "\n",
    "hyperopt_search = \"FIX ME\"  # TODO: Change this\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    num_samples=10,  \n",
    "    search_alg=\"FIX ME\",  #  TODO: Change this\n",
    "    **experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please: fill out this form to provide feedback on this tutorial!\n",
    "\n",
    "https://goo.gl/forms/NVTFjUKFz4TH8kgK2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
