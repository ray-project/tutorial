{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Accelerated Hyperparameter Tuning For PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this tutorial, we'll show you how to leverage advanced hyperparameter tuning techniques with Tune.\n",
    "\n",
    "<img src=\"tune-arch-simple.png\" alt=\"Tune Logo\" width=\"600\"/>\n",
    "\n",
    "Specifically, we'll leverage ASHA and Bayesian Optimization (via HyperOpt) without modifying your underlying code.\n",
    "\n",
    "Tune is a scalable framework for model training and hyperparameter search with a focus on deep learning and deep reinforcement learning.\n",
    "\n",
    "* **Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune \n",
    "* **Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "* **Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "* **Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are running on Google Colab, uncomment below to install the necessary dependencies \n",
    "## before beginning the exercise.\n",
    "\n",
    "# !pip uninstall -y pyarrow\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-0.8.0.dev5-cp37-cp37m-manylinux1_x86_64.whl\n",
    "# !pip install ray[debug]\n",
    "\n",
    "# # A hack to force the runtime to restart, needed to include the above dependencies.\n",
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: PyTorch Boilerplate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from ray.tune.examples.mnist_pytorch import train, test, ConvNet, get_data_loaders\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have some boiler plate code for a PyTorch training function. You can take a look at these functions. For example, `train` is simply a for loop over the data loader.\n",
    "\n",
    "```python\n",
    "    def train(model, optimizer, train_loader):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if batch_idx * len(data) > EPOCH_SIZE:\n",
    "                return\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "\n",
    "In order to make decisions in the middle of training, we need to let the training function notify Tune. The ``tune.track`` API allows Tune to keep track of current results.\n",
    "\n",
    "**TODO**: Add `tune.track.log(mean_accuracy=acc)` within the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config):\n",
    "    model = ConvNet()\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "\n",
    "    for i in range(20):\n",
    "        train(model, optimizer, train_loader)  # Train for 1 epoch\n",
    "        acc = test(model, test_loader)  # Obtain validation accuracy.\n",
    "        # TODO: Add tune.track.log(mean_accuracy=acc) here\n",
    "        if i % 5 == 0:\n",
    "            torch.save(model, \"./model.pth\") # This saves the model to the trial directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Trial Run\n",
    "\n",
    "Let's run 1 trial, randomly sampling from a uniform distribution for learning rate and momentum. \n",
    "\n",
    "A \"trial\" is the execution of training using a set of hyperparameters. An **experiment** is a set of trials (i.e., a hyperparameter search).\n",
    "\n",
    "Run the below cell to run Tune. \n",
    "\n",
    "#### This is one random sample and should perform poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.1, 0.9)\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    config=search_space, \n",
    "    verbose=1,\n",
    "    name=\"train_mnist\",  # This is used to specify the logging directory.\n",
    "    stop={\"mean_accuracy\": 0.98}  # This will stop the trial \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the performance of this trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = analysis.fetch_trial_dataframes()\n",
    "[d.mean_accuracy.plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Efficient Grid Search with Early Stopping\n",
    "\n",
    "\n",
    "Tune provides a `tune.grid_search` primitive to pass into `tune.run` as follows:\n",
    "```python\n",
    "tune.run(config={\"variable\": tune.grid_search([1, 2, 3])})\n",
    "```\n",
    "\n",
    "From this, Tune will run 3 trials, evaluating each value in the grid search. To specify a multi-dimensional grid search, you can use `tune.grid_search` on multiple variables:\n",
    "\n",
    "\n",
    "```python\n",
    "tune.run(config={\n",
    "    \"variable1\": tune.grid_search([1, 2, 3]),\n",
    "    \"variable2\": tune.grid_search([1, 2, 3]),\n",
    "    \"variable3\": tune.grid_search([1, 2, 3]),\n",
    "    \"variable4\": tune.grid_search([1, 2, 3]),\n",
    "})\n",
    "```\n",
    "\n",
    "This will generate a total $3 * 3 * 3 * 3 = 81$ trials.\n",
    "\n",
    "**TODO**: Specify a multi-dimensional grid search, gridding over `lr` and `momentum`. Choose 5 values between 0.001 to 0.9 for both values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify a multi-dimensional grid search, gridding over lr and momentum. \n",
    "# Choose 5 values between 0.001 to 0.9 for both values.\n",
    "hyperparameter_space = {\n",
    "    \"lr\": \"TODO\"\n",
    "    \"momentum\":  \"TODO\"\n",
    "}\n",
    "\n",
    "assert \"grid_search\" in hyperparameter_space.get(\"lr\") \n",
    "assert \"grid_search\" in hyperparameter_space.get(\"momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an early-stopping algorithm\n",
    "\n",
    "An efficient hyperparameter optimization avoids training low-performing trials. This is one of the main inefficiencies of a grid search. \n",
    "\n",
    "In Tune, we can avoid this by using state-of-the-art search algorithms such as ASHA. ASHA is a scalable algorithm for principled early stopping. How does it work? On a high level, it terminates trials that are less promising and allocates more time and resources to more promising trials. \n",
    "\n",
    "    The successive halving algorithm begins with all candidate configurations in the base rung and proceeds as follows:\n",
    "\n",
    "        1. Uniformly allocate a budget to a set of candidate hyperparameter configurations in a given rung.\n",
    "        2. Evaluate the performance of all candidate configurations.\n",
    "        3. Promote the top half of candidate configurations to the next rung.\n",
    "        4. Double the budget per configuration for the next rung and repeat until one configurations remains. \n",
    "        \n",
    "A textual representation:\n",
    "    \n",
    "           | Configurations | Epochs per \n",
    "           | Remaining      | Configuration\n",
    "    ---------------------------------------\n",
    "    Rung 1 | 27             | 1\n",
    "    Rung 2 | 9              | 3\n",
    "    Rung 3 | 3              | 9\n",
    "    Rung 4 | 1              | 27\n",
    "\n",
    "(from https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Set up ASHA.\n",
    "\n",
    "1) Create an ASHA \"Scheduler\" (ASHA). A scheduler decides which trials to run, stop, or pause. \n",
    "```python\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "custom_scheduler = ASHAScheduler(\n",
    "    metric='mean_accuracy',\n",
    "    mode=\"max\",\n",
    "    grace_period=1,\n",
    ")\n",
    "```\n",
    "\n",
    "*Note: Read the documentation on this step at https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband or call ``help(tune.schedulers.ASHAScheduler)`` to learn more about the ASHA Scheduler*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### How do I debug things in Tune?\n",
    "\n",
    "The `error file` column will show up in the output. Run the below cell with the ``error file`` path to diagnose your issue.\n",
    "\n",
    "```\n",
    "! cat /home/ubuntu/tune_iris/tune_iris_c66e1100_2019-10-09_17-13-24x_swb9xs/error_2019-10-09_17-13-29.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "custom_scheduler = None\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    scheduler=custom_scheduler, \n",
    "    config=hyperparameter_space, \n",
    "    verbose=1,\n",
    "    name=\"train_mnist\"  # This is used to specify the logging directory.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's plot our results by wall-clock time and epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by wall-clock time\n",
    "\n",
    "dfs = analysis.fetch_trial_dataframes()\n",
    "# This plots everything on the same plot\n",
    "ax = None\n",
    "for d in dfs.values():\n",
    "    ax = d.plot(\"timestamp\", \"mean_accuracy\", ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot by epoch\n",
    "ax = None\n",
    "for d in dfs.values():\n",
    "    ax = d.mean_accuracy.plot(ax=ax, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Search Algorithms in Tune\n",
    "\n",
    "Tune enables you to scale existing hyperparameter search libraries such as HyperOpt (https://github.com/hyperopt/hyperopt). In this setting, use the external library's hyperparameter space specification instead of Tune's configuration.\n",
    "\n",
    "Search algorithms can limit the number of concurrent hyperparameters are being evaluated. This is necessary because sometimes the external library is more effective when evaluations are sequential.\n",
    "\n",
    "**TODO:** Create a HyperOptSearch object by passing in a HyperOpt specific search space. Also enforce that only 2 trials can run concurrently:\n",
    "\n",
    "```python\n",
    "    hyperopt_search = HyperOptSearch(space, max_concurrent=2, metric=\"mean_accuracy\", mode=\"max\")\n",
    "```\n",
    "\n",
    "Then, plug in `HyperOptSearch` into `tune.run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "# This is a HyperOpt specific hyperparameter space configuration.\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", -10, -1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "}\n",
    "\n",
    "# TODO: Create a HyperOptSearch object by passing in a HyperOpt specific search space.\n",
    "# Also enforce that only 2 trials can run concurrently:\n",
    "hyperopt_search = \"TODO\" # TODO: Change this\n",
    "\n",
    "\n",
    "! rm -rf ~/ray_results/search_algorithm\n",
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    num_samples=10,  \n",
    "    search_alg=\"TODO\",  #  TODO: Change this\n",
    "    verbose=1,\n",
    "    name=\"search_algorithm\"  # This is used to specify the logging directory.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra - use Tensorboard for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use TensorBoard to view trial performances. If the graphs do not load, click `Toggle All Runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ~/ray_results/search_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please: fill out this form to provide feedback on this tutorial!\n",
    "\n",
    "https://goo.gl/forms/NVTFjUKFz4TH8kgK2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your machine has a GPU, you can use the `resources_per_trial` argument to specify that the trial should use a GPU. This allows Tune to automatically set the `CUDA_VISIBLE_DEVICES` for the trial and enforce resource isolation (i.e., 1 trial per GPU at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    train_mnist, \n",
    "    num_samples=10,  \n",
    "    resources_per_trial={\"GPU\": 1},\n",
    "    search_alg=\"TODO\",  #  TODO: Change this\n",
    "    verbose=1,\n",
    "    name=\"search_algorithm\"  # This is used to specify the logging directory.\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
