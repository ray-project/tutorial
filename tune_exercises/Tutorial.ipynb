{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tune.png\" alt=\"Tune Logo\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is a scalable framework for model training and hyperparameter search with a focus on deep learning and deep reinforcement learning.\n",
    "\n",
    "**Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune\n",
    "\n",
    "**Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "\n",
    "**Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "\n",
    "**Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Tuning hyperparameters is often the most expensive part of the machine learning workflow. Tune is built to address this, demonstrating an efficient and scalable solution for this pain point.\n",
    "\n",
    "This tutorial will walk you through the following process:\n",
    "\n",
    "1. Creating and training a model on a toy dataset (MNIST)\n",
    "2. Integrating Tune into your workflow by creating a trainable and running an experiment\n",
    "3. Trying out advanced features - plugging in an efficient scheduler\n",
    "4. (Optional) Try out a search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating and training an un-Tune-d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from model import load_data, make_model, evaluate\n",
    "from helper import prepare_data\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and train a model to classify [MNIST](https://www.wikiwand.com/en/MNIST_database) digits **without tuning**, as a baseline first model. We will be creating a Convolutional Nueral Network model (using [Keras](https://keras.io/)) to classify the digits. \n",
    "\n",
    "*Note: If you would like to see the specifics of the `load_data`, `make_model`, and `evaluate`, feel free to check out model.py!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist():\n",
    "    data_generator = load_data()\n",
    "    model = make_model()\n",
    "    for batch_of_data, batch_of_labels in data_generator:\n",
    "        model.fit(batch_of_data, batch_of_labels, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model. (This should take ~30 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_model = train_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate the un-Tune-d model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(first_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now quickly try out this model to see if it works as expected. We'll load the model with our trained weights. Try to write a digit into the box below. This will automatically save your input in a variable `data` behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = None\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tip: don't expect it to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_data(data)\n",
    "print(\"This model predicted your input as\", first_model.predict(prepared_data).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from helper import test_reporter, get_best_result\n",
    "from model import load_data, make_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we might want to do now is find better hyperparameters so that our model trains more quickly and possibly to a higher accuracy. Let's make some minor modifications to utilize Tune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune uses Ray as a backend, so we will first import and initialize Ray. You can ignore the output at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Defining a Trainable to run\n",
    "\n",
    "Tune will automate and distribute your hyperparameter search by scheduling a number of **trials** on a machine. Each trial runs a user-defined Python function with a sampled set of hyperparameters called a **trainable**. \n",
    "\n",
    "We define a new training function `train_mnist_tune` as our trainable. A trainable must pass in a `reporter` object, train the model, and report some metric(s) to Tune. This allows Tune to keep track of performance as the model is training.\n",
    "\n",
    "**TODO: After fitting the Keras model, get the `mean_accuracy` from Keras, and call the ``reporter`` to report the `mean_accuracy` for every batch**. \n",
    "\n",
    "You can get model accuracy from the Keras model with the following code:\n",
    "\n",
    "```python\n",
    "mean_accuracy = model.evaluate(x_batch, y_batch)[1]\n",
    "```\n",
    "\n",
    "\n",
    "Example of using the reporter:\n",
    "\n",
    "```python\n",
    "reporter(mean_accuracy=accuracy, metric2=1, metric3=0.3, ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_tune(config, reporter):\n",
    "    data_generator = load_data()\n",
    "    model = make_model()\n",
    "    for i, (x_batch, y_batch) in enumerate(data_generator):\n",
    "        model.fit(x_batch, y_batch, verbose=0)\n",
    "        if i % 3 == 0:\n",
    "            last_checkpoint = \"weights_tune_{}.h5\".format(i)\n",
    "            model.save_weights(last_checkpoint)\n",
    "        \n",
    "        reporter(mean_accuracy=None, # TODO\n",
    "                 timesteps_total=i, \n",
    "                 checkpoint=last_checkpoint)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take 30 seconds or so to run if incorrectly written\n",
    "assert test_reporter(train_mnist_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Call ``help(tune.trainable)`` if you are interested in learning more about what qualifies as trainable in Tune.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure the search and run Tune\n",
    "\n",
    "Now that we have a working trainable, we can configure the search. We will use some basic Tune features for training - namely specifying a stopping criteria and a search space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First, set the stopping criteria to when `mean_accuracy` passes `0.95`. For example, to specify that trials will be stopped whenever they report a `mean_accuracy` that is `>= 0.95`, do:\n",
    "\n",
    "```python\n",
    "stop={\"mean_accuracy\": 0.95}\n",
    "```\n",
    "\n",
    "\n",
    "2) We also want to designate a search space. We'll search over *learning rate*, which sets the step size of our model update, and *momentum*, which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "\n",
    "For `learning rate`, Tune supports sampling parameters from user-specified lambda functions, which can be used independently or in combination with grid search. For `momentum`, you can use `tune.grid_search` to specify an axis of a grid search. For example:\n",
    "\n",
    "```python\n",
    "space={\n",
    "    \"lr\": tune.sample_from(lambda spec: np.random.uniform(0.001, 0.1)),\n",
    "    \"momentum\": tune.grid_search([0.2, 0.4, 0.6]),\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**TODO: Configure `tune.run` with a stopping criteria using `stop` and a search space using `config`**\n",
    "\n",
    "As a reminder, we want to stop when `mean_accuracy` passes `0.95` and randomly search for learning rate `\"lr\"` between 0.001 to 0.1 with a grid search over `\"momentum\"` for `[0.2, 0.4, 0.6]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, run the experiment! (this should take ~1 minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"lr\": None, # TODO\n",
    "    \"momentum\": None # TODO\n",
    "}\n",
    "trials = tune.run(\n",
    "    train_mnist_tune,\n",
    "    stop=None, # TODO\n",
    "    config=space,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect the result below to be about `0.6`, although your mileage may vary (and it's OK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best result is\", get_best_result(trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy is still low, similar to the accuracy of our first un-Tune-d model! In the next section, we will scale up the search and accelerate the training using a state of the art algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Call ``help(tune.run)`` if you are interested in learning more about executing experiments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scale up the search with more samples, hyperparameters, and a custom scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import AsyncHyperBandScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Tune schedules trials in serial order with the `FIFOScheduler` class. Instead, we can specify a custom scheduling algorithm, such as `HyperBand`, to scale up and accelerate our training. `Hyperband` is state of the art algorithm that focuses on speeding up random search through adaptive resource allocation and early-stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a few steps to scale up our search and add `HyperBand`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Sample the search space 5 times using the parameter `num_samples`. For example,\n",
    "\n",
    "```python\n",
    "num_samples=5\n",
    "```\n",
    "\n",
    "\n",
    "2) In addition to `learning rate` and `momentum`, search over another hyperparameter `\"hidden\"` from 16 to 512 which specifies the size of the last neural network layer.\n",
    "\n",
    "Here, use `np.random.randint`. For example,\n",
    "\n",
    "```python\n",
    "config={\n",
    "    ...\n",
    "    \"hidden\": tune.sample_from(lambda spec: np.random.randint(16, 512))\n",
    "}\n",
    "```\n",
    "\n",
    "3) Create an Asynchronous HyperBand Scheduler. Be sure to set the `time_attr` to `timesteps_total` and `reward_attr` to `mean_accuracy`. For example,\n",
    "\n",
    "```python\n",
    "custom_scheduler = AsyncHyperBandScheduler(time_attr='timesteps_total', reward_attr='mean_accuracy')\n",
    "```\n",
    "\n",
    "*Note: Read the documentation on this step at https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband or call ``help(tune.schedulers.AsyncHyperBandScheduler)`` to learn more about the Asynchronous Hyperband Scheduler*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Create and specify the Hyperband scheduler using `scheduler`, search over another hyperparameter `hidden`, and specify the number of sample with `num_samples`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, run the experiment! (this should take ~1 minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_scheduler = None # TODO\n",
    "space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: np.random.uniform(0.001, 0.1)),\n",
    "    \"momentum\": tune.grid_search([0.2, 0.4, 0.6]),\n",
    "    \"hidden\": None # TODO\n",
    "}\n",
    "better_trials = tune.run(\n",
    "    train_mnist_tune,\n",
    "    num_samples=None, # TODO\n",
    "    scheduler=custom_scheduler,\n",
    "    stop={\"mean_accuracy\": 0.95},\n",
    "    config=space,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can expect the result to be about `0.95`, although your mileage may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The best result is\", get_best_result(better_trials, metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Congratulations, you're now a Tune expert! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please: fill out this form to provide feedback on this tutorial!\n",
    "\n",
    "https://goo.gl/forms/NVTFjUKFz4TH8kgK2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Try using a search algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune is an execution layer, so we can combine powerful optimizers such as HyperOpt (https://github.com/hyperopt/hyperopt) with state-of-the-art algorithms such as HyperBand without modifying any model training code.\n",
    "\n",
    "The documentation to doing this is here: https://ray.readthedocs.io/en/latest/tune-searchalg.html#hyperopt-search-tree-structured-parzen-estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.uniform(\"lr\", 0.001, 0.1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.1, 0.9),\n",
    "    \"hidden\": hp.choice(\"hidden\", np.arange(16, 256, dtype=int)),\n",
    "}\n",
    "\n",
    "hyperband = AsyncHyperBandScheduler(time_attr='timesteps_total', reward_attr='mean_accuracy')\n",
    "\n",
    "hyperopt_search = HyperOptSearch(space, reward_attr=\"mean_accuracy\")\n",
    "\n",
    "good_results = tune.run(\n",
    "    train_mnist_tune,\n",
    "    num_samples=5,\n",
    "    search_alg=hyperopt_search,\n",
    "    scheduler=hyperband,\n",
    "    stop={\"mean_accuracy\": 0.95},\n",
    "    config=space,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The best result is\", get_best_result(good_results, metric=\"mean_accuracy\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
