{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Learning how to use Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tune.png\" alt=\"Tune Logo\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Tuning hyperparameters is often the most expensive part of the machine learning workflow. Tune is built to address this, demonstrating an efficient and scalable solution for this pain point.\n",
    "\n",
    "**Code**: https://github.com/ray-project/ray/tree/master/python/ray/tune\n",
    "\n",
    "**Examples**: https://github.com/ray-project/ray/tree/master/python/ray/tune/examples\n",
    "\n",
    "**Documentation**: http://ray.readthedocs.io/en/latest/tune.html\n",
    "\n",
    "**Mailing List** https://groups.google.com/forum/#!forum/ray-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will step through a couple key steps of the hyperparameter tuning process with Tune. \n",
    "\n",
    "1. Visualizing the data.\n",
    "2. Creating a model training procedure (using Keras).\n",
    "3. Tuning the model by adapting the above model training procedure to **use Tune**.\n",
    "4. Analyzing the model created by Tune.\n",
    "\n",
    "Note that this uses Tune's **function-based API**. This is mainly for prototyping. A later tutorial will cover Tune's more powerful **class-based Trainable** API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.integration.keras import TuneReporterCallback\n",
    "from ray.tune.examples.utils import get_iris_data\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the distribution of the dataset.\n",
    "\n",
    "### The goal of this tutorial is to have a model that can accurately predict the true label given a tuple of sepal length, petal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "true_data = iris['data']\n",
    "true_label = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "def plot_data(X, y):\n",
    "    # Visualize the data sets\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for target, target_name in enumerate(names):\n",
    "        X_plot = X[y == target]\n",
    "        plt.plot(X_plot[:, 0], X_plot[:, 1], linestyle='none', marker='o', label=target_name)\n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.axis('equal')\n",
    "    plt.legend();\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for target, target_name in enumerate(names):\n",
    "        X_plot = X[y == target]\n",
    "        plt.plot(X_plot[:, 2], X_plot[:, 3], linestyle='none', marker='o', label=target_name)\n",
    "    plt.xlabel(feature_names[2])\n",
    "    plt.ylabel(feature_names[3])\n",
    "    plt.axis('equal')\n",
    "    plt.legend();\n",
    "    \n",
    "plot_data(true_data, true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model training procedure (using Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a function that will take in some hyperparameters and return a model that we can then use to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, dense_1, dense_2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(dense_1), input_shape=(4,), activation='relu', name='fc1'))\n",
    "    model.add(Dense(int(dense_2), activation='relu', name='fc2'))\n",
    "    model.add(Dense(3, activation='softmax', name='output'))\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function that trains the model using the ``create_model`` function and returns the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_iris():\n",
    "    train_x, train_y, test_x, test_y = get_iris_data()\n",
    "    model = create_model(learning_rate=0.1, dense_1=2, dense_2=2)\n",
    "    # This saves the top model\n",
    "    checkpoint_callback = ModelCheckpoint(\"model.h5\", monitor='val_loss', save_best_only=True, period=3)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_x, train_y, \n",
    "        validation_data=(test_x, test_y),\n",
    "        verbose=0, batch_size=5, epochs=20, callbacks=[checkpoint_callback])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly train the model on the dataset. The accuracy should be quite low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = train_on_iris()  # This trains the model and returns it.\n",
    "train_x, train_y, test_x, test_y = get_iris_data()\n",
    "original_loss, original_accuracy = original_model.evaluate(test_x, test_y)\n",
    "print(\"Loss is {:0.4f}\".format(original_loss))\n",
    "print(\"Accuracy is {:0.4f}\".format(original_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with Tune\n",
    "\n",
    "Now, let's use Tune to optimize a model that learns to classify Iris. This will happen in two parts - **modifying** the training function to support Tune, and then **configuring** Tune.\n",
    "\n",
    "### Integration Part 1: Modifying the Training Function\n",
    "\n",
    "**Instructions** Follow the next 2 steps for modifying the ``train_iris`` function to support Tune.\n",
    "\n",
    "1. Change the signature of the function to take in a configuration dictionary.\n",
    "\n",
    "```python\n",
    "def tune_iris(config)\n",
    "```\n",
    "    \n",
    "    \n",
    "2. Pass in the configuration values into ``create_model``:\n",
    "\n",
    "```python\n",
    "model = create_model(learning_rate=config[\"lr\"], dense_1=config[\"dense_1\"], dense_2=config[\"dense_2\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_iris():  # TODO: Change me.\n",
    "    train_x, train_y, test_x, test_y = get_iris_data()\n",
    "    model = create_model(learning_rate=0, dense_1=0, dense_2=0)  # TODO: Change me.\n",
    "    checkpoint_callback = ModelCheckpoint(\"model.h5\", monitor='val_loss', save_best_only=True, period=3)\n",
    "\n",
    "    # Enable Tune to make intermediate decisions by using a Tune Callback hook. This is Keras specific.\n",
    "    callbacks = [checkpoint_callback, TuneReporterCallback(freq=\"epoch\")]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_x, train_y, \n",
    "        validation_data=(test_x, test_y),\n",
    "        verbose=0, \n",
    "        batch_size=5, \n",
    "        epochs=20, \n",
    "        callbacks=callbacks)\n",
    "    \n",
    "assert len(inspect.getargspec(tune_iris).args) == 1, \"The `tune_iris` function needs to take in the arg `config`.\"\n",
    "\n",
    "print(\"Test-running to make sure this function will run correctly.\")\n",
    "tune.track.init()  # For testing purposes only.\n",
    "tune_iris({\"lr\": 0.1, \"dense_1\": 4, \"dense_2\": 4})\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Part 2: Configuring Tune to tune hyperparameters.\n",
    "\n",
    "**Instructions** Follow the next 2 steps to configure Tune to identify the top hyperparameters.\n",
    "\n",
    "1. Designate the hyperparameter space. \n",
    "\n",
    "```python\n",
    "hyperparameter_space = {\n",
    "    \"lr\": tune.loguniform(0.0001, 0.1),  \n",
    "    \"dense_1\": tune.uniform(2, 64),\n",
    "    \"dense_2\": tune.uniform(2, 64),\n",
    "}\n",
    "```\n",
    "2. Increase the number of samples. \n",
    "\n",
    "```python\n",
    "num_samples = 20\n",
    "```\n",
    "\n",
    "#### How does parallelism work in Tune?\n",
    "\n",
    "\n",
    "Setting ``num_samples`` will run a *total* of 20 trials (hyperparameter configuration samples). However, not all of them will run at once. The max training concurrency will be the number of CPU cores on the machine you're running on. For a 2-core machine, 2 models will be trained concurrently. When one is finished, a new training process will start with a new hyperparameter configuration sample. \n",
    "\n",
    "Each trial will run on a new Python process. The python process is killed when the trial is finished.\n",
    "\n",
    "\n",
    "#### How do I debug things in Tune?\n",
    "\n",
    "The `error file` column will show up in the output. Run the below cell with the ``error file path`` path to diagnose your issue.\n",
    "\n",
    "```\n",
    "! cat /home/ubuntu/tune_iris/tune_iris_c66e1100_2019-10-09_17-13-24x_swb9xs/error_2019-10-09_17-13-29.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparameter_space = {}  # TODO: Fill me out.\n",
    "num_samples = 1  # TODO: Fill me out.\n",
    "\n",
    "####################################################################################################\n",
    "################ This is just a validation function for tutorial purposes only. ####################\n",
    "HP_KEYS = [\"lr\", \"dense_1\", \"dense_2\"]\n",
    "assert all(key in hyperparameter_space for key in HP_KEYS), (\n",
    "    \"The hyperparameter space is not fully designated. It must include all of {}\".format(HP_KEYS))\n",
    "######################################################################################################\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune_iris, \n",
    "    verbose=1,\n",
    "    config=hyperparameter_space,\n",
    "    num_samples=num_samples)\n",
    "\n",
    "assert len(analysis.trials) == 20, \"Did you set the correct number of samples?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the best tuned model\n",
    "\n",
    "Let's compare the real labels with the classified labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_data, test_labels = get_iris_data()\n",
    "plot_data(test_data, test_labels.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the directory where the best model is saved.\n",
    "logdir = analysis.get_best_logdir(\"keras_info/val_loss\", mode=\"min\")\n",
    "\n",
    "# We saved the model as `model.h5` in the logdir of the trial.\n",
    "from keras.models import load_model\n",
    "tuned_model = load_model(logdir + \"/model.h5\")\n",
    "\n",
    "tuned_loss, tuned_accuracy = tuned_model.evaluate(test_data, test_labels)\n",
    "print(\"Loss is {:0.4f}\".format(tuned_loss))\n",
    "print(\"Tuned accuracy is {:0.4f}\".format(tuned_accuracy))\n",
    "print(\"The original un-tuned model had an accuracy of {:0.4f}\".format(original_accuracy))\n",
    "predicted_label = tuned_model.predict(test_data)\n",
    "plot_data(test_data, predicted_label.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra - use Tensorboard for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use TensorBoard to view trial performances. If the graphs do not load, click `Toggle All Runs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ~/ray_results/tune_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
