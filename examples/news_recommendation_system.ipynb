{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple news recommendation system\n",
    "\n",
    "The goal of this example is to train a very simple news recommendation system. We will\n",
    "- prepare the training data in parallel with Ray\n",
    "- train a simple model that classifies article titles as \"popular\" or \"less popular\" using scikit learn and\n",
    "- find good hyperparameter settings for the model with Tune, Ray's parallel hyperparameter optimization library.\n",
    "\n",
    "### Downloading and preparing the training data\n",
    "\n",
    "First we will download and decompress 400,000 hackernews submissions. This is a small subset of the articles that have been submitted to https://news.ycombinator.com. The data includes the title of each submission and its score, which roughly corresponds to the number of upvotes. There are 4 batches of JSON files that contain the information, named `submission-1.json` through `submission-4.json`. The first couple lines of the first file will be printed below by the `head` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://s3-us-west-2.amazonaws.com/ray-tutorials/hackernews.zip\n",
    "!unzip -o hackernews.zip\n",
    "!head -n 2 submission-1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=4, include_webui=False, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below parses a chunk of the data and produces a pandas DataFrame with the titles and scores of the submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hn_submissions(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        records = []\n",
    "        for line in f.readlines():\n",
    "            body = json.loads(line)[\"body\"]\n",
    "            records.append({\"data\": body[\"title\"], \"score\": body[\"score\"]})\n",
    "        return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now process all the data chunks and concatenate them into a single dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "files = [\"submission-\" + str(i) + \".json\" for i in range(1, 5)]\n",
    "records = [parse_hn_submissions(file) for file in files]\n",
    "df = pd.concat(records)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(\"Took {} seconds to parse the hackernews submissions\".format(duration))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Parallelize the parsing of the four files with Ray!\n",
    "\n",
    "**Note**: In Binder this will not actually lead to a speedup due to constrained resources (because each Binder instance is shared with many other people). On an uncontended EC2 instance, we get **4.25s** for the serial code and **1.34s** for the parallel version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following lines to determine a cutoff of what we consider a \"good\" article. The median score for articles is 1, so we want to label articles with score higher than that as class \"1\" and everything else as \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"score\"] > 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done preparing the data and can start training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "First we split the data into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines a pipeline that first converts the title of the submission to a bag of words and then applies an SVM for the actual classification. Note that we are fitting a very simple SVM here due to the computational restrictions of Binder. With more resources, a state-of-the-art model like [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) would be a better choice, but a code structure would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=0.001,\n",
    "                          max_iter=5, tol=1e-3))])\n",
    "result = pipeline.fit(train.data, train.target)\n",
    "\n",
    "predicted = result.predict(train.data)\n",
    "print(\"Accuracy on the training set is {}\".format(np.mean(predicted == train.target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pipeline.predict(test.data)\n",
    "print(\"Accuracy on the test set is {}\".format(np.mean(predicted == test.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also classify new titles as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict([\"Iconic consoles of the IBM System/360 mainframes, 55 years old today\",\n",
    "                  \"Are Banned Drugs in Your Meat?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to improve these results by doing some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to put the training data into the object store (to make sure it will be re-used between training runs), and define the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = ray.put(train)\n",
    "test_id = ray.put(test)\n",
    "\n",
    "def train_func(config, reporter):\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=config[\"alpha\"],\n",
    "                              max_iter=1, tol=1e-3,\n",
    "                              warm_start=True))])\n",
    "    \n",
    "    train = ray.get(train_id)\n",
    "    test = ray.get(test_id)\n",
    "    for i in range(5):\n",
    "        # Perform one epoch of SGD\n",
    "        X = pipeline.named_steps[\"vect\"].fit_transform(train.data)\n",
    "        pipeline.named_steps[\"clf\"].partial_fit(X, train.target, classes=[0, 1])\n",
    "        with open(\"model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "        reporter(mean_accuracy=np.mean(pipeline.predict(test.data) == test.target)) # report metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the best setting for the regularization parameter $\\alpha$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials = tune.run(\n",
    "    train_func,\n",
    "    name=\"news_recommendation\",\n",
    "    # With the \"stop\" parameter, you could also specify a stopping criterion.\n",
    "    config={\"alpha\": tune.grid_search([1e-3, 1e-4, 1e-5, 1e-6])},\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model can now be loaded and evaluated like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_index = np.argmax([trial.last_result[\"mean_accuracy\"] for trial in all_trials])\n",
    "best_result_path = os.path.join(all_trials[best_result_index].logdir, \"model.pkl\")\n",
    "with open(best_result_path, \"rb\") as f:\n",
    "    pipeline = pickle.load(f)\n",
    "print(\"Best result was {}\".format(np.mean(pipeline.predict(test.data) == test.target)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
