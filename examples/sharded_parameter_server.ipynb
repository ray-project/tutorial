{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharded Parameter Servers\n",
    "\n",
    "**GOAL:** The goal of this exercise is to use actor handles to implement a sharded parameter server example for **distributed asynchronous stochastic gradient descent**.\n",
    "\n",
    "Before doing this exercise, make sure you understand the concepts from the exercise on **Actor Handles**.\n",
    "\n",
    "### Parameter Servers\n",
    "\n",
    "A parameter server is simply an object that stores the parameters (or \"weights\") of a machine learning model (this could be a neural network, a linear model, or something else). It exposes two methods: one for getting the parameters and one for updating the parameters.\n",
    "\n",
    "In a typical machine learning training application, worker processes will run in an infinite loop that does the following:\n",
    "1. Get the latest parameters from the parameter server.\n",
    "2. Compute an update to the parameters (using the current parameters and some data).\n",
    "3. Send the update to the parameter server.\n",
    "\n",
    "The workers can operate synchronously (that is, in lock step), in which case distributed training with multiple workers is algorithmically equivalent to serial training with a larger batch of data. Alternatively, workers can operate independently and apply their updates asynchronously. The main benefit of asynchronous training is that a single slow worker will not slow down the other workers. The benefit of synchronous training is that the algorithm behavior is more predictable and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple parameter server can be implemented as a Python class in a few lines of code.\n",
    "\n",
    "**EXERCISE:** Make the `ParameterServer` class an actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "\n",
    "class ParameterServer(object):\n",
    "    def __init__(self, dim):\n",
    "        self.parameters = np.zeros(dim)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    def update_parameters(self, update):\n",
    "        self.parameters += update\n",
    "\n",
    "\n",
    "ps = ParameterServer(dim)\n",
    "\n",
    "assert hasattr(ParameterServer, 'remote'), ('You need to turn ParameterServer into an '\n",
    "                                            'actor (by using the ray.remote keyword).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A worker can be implemented as a simple Python function that repeatedly gets the latest parameters, computes an update to the parameters, and sends the update to the parameter server.\n",
    "\n",
    "**EXERCISE:** Make the `worker` function a remote function. Since you turned `ParameterServer` into an actor class above, you'll also need to modify the `get_parameters` and `update_parameters` function invocations (by adding the `.remote` keyword). You'll also need to call `ray.get` to get the result of `get_parameters` (but not for `update_parameters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(ps, dim, num_iters):\n",
    "    for _ in range(num_iters):\n",
    "        # Get the latest parameters.\n",
    "        parameters = ps.get_parameters()\n",
    "        # Compute an update.\n",
    "        update = 1e-3 * parameters + np.ones(dim)\n",
    "        # Update the parameters.\n",
    "        ps.update_parameters(update)\n",
    "        # Sleep a little to simulate a real workload.\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Test that worker is implemented correctly. You do not need to change this line.\n",
    "ray.get(worker.remote(ps, dim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start two workers.\n",
    "worker_results = [worker(ps, dim, 100) for _ in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the worker tasks are executing, you can query the parameter server from the driver and see the parameters changing in the background.\n",
    "\n",
    "**EXERCISE:** Experiment by querying the parameter server below and see how the parameters change. Simply run the next cell a bunch of times and watch the values change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.get(ps.get_parameters.remote()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharding a Parameter Server\n",
    "\n",
    "As the number of workers increases, the volume of updates being sent to the parameter server will increase. At some point, the network bandwidth into the parameter server machine or the computation down by the parameter server may be a bottleneck.\n",
    "\n",
    "Suppose you have $N$ workers and $1$ parameter server, and suppose each of these is an actor that lives on its own machine. Furthermore, suppose the model size is $M$ bytes. Then sending all of the parameters from the workers to the parameter server will mean that $N * M$ bytes in total are sent to the parameter server. If $N = 100$ and $M = 10^8$, then the parameter server must receive ten gigabytes, which, assuming a network bandwidth of 10 giga*bits* per second, would take 8 seconds. This would be prohibitive.\n",
    "\n",
    "On the other hand, if the parameters are sharded (that is, split) across `K` parameter servers, `K` is `100`, and each parameter server lives on a separate machine, then each parameter server needs to receive only 100 megabytes, which can be done in 80 milliseconds. This is much better.\n",
    "\n",
    "**EXERCISE:** The code below defines a parameter server shard class. Modify this class to make `ParameterServerShard` an actor. We will need to revisit this code soon and increase `num_shards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterServerShard(object):\n",
    "    def __init__(self, sharded_dim):\n",
    "        self.parameters = np.zeros(sharded_dim)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    def update_parameters(self, update):\n",
    "        self.parameters += update\n",
    "\n",
    "\n",
    "total_dim = (10 ** 8) // 8  # This works out to 100MB (we have 25 million\n",
    "                            # float64 values, which are each 8 bytes).\n",
    "num_shards = 1  # The number of parameter server shards.\n",
    "\n",
    "assert total_dim % num_shards == 0, ('In this exercise, the number of shards must '\n",
    "                                     'perfectly divide the total dimension.')\n",
    "\n",
    "# Start some parameter servers.\n",
    "ps_shards = [ParameterServerShard(total_dim // num_shards) for _ in range(num_shards)]\n",
    "\n",
    "assert hasattr(ParameterServerShard, 'remote'), ('You need to turn ParameterServerShard into an '\n",
    "                                                 'actor (by using the ray.remote keyword).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a worker that does the following.\n",
    "1. Gets the latest parameters from all of the parameter server shards.\n",
    "2. Concatenates the parameters together to form the full parameter vector.\n",
    "3. Computes an update to the parameters.\n",
    "4. Partitions the update into one piece for each parameter server.\n",
    "5. Applies the right update to each parameter server shard.\n",
    "\n",
    "**EXERCISE:** Modify the code below to make `worker_task` a remote function. You will also need to modify the parameter server method invocations within the function (e.g., by adding `.remote` where needed and calling `ray.get` when needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_task(total_dim, num_iters, *ps_shards):\n",
    "    # Note that ps_shards are passed in using Python's variable number\n",
    "    # of arguments feature. We do this because currently actor handles\n",
    "    # cannot be passed to tasks inside of lists or other objects.\n",
    "    for _ in range(num_iters):\n",
    "        # Get the current parameters from each parameter server.\n",
    "        parameter_shards = [ps.get_parameters() for ps in ps_shards]\n",
    "        assert all([isinstance(shard, np.ndarray) for shard in parameter_shards]), (\n",
    "               'The parameter shards must be numpy arrays. Did you forget to call ray.get?')\n",
    "        # Concatenate them to form the full parameter vector.\n",
    "        parameters = np.concatenate(parameter_shards)\n",
    "        assert parameters.shape == (total_dim,)\n",
    "\n",
    "        # Compute an update.\n",
    "        update = np.ones(total_dim)\n",
    "        # Shard the update.\n",
    "        update_shards = np.split(update, len(ps_shards))\n",
    "        \n",
    "        # Apply the updates to the relevant parameter server shards.\n",
    "        for ps, update_shard in zip(ps_shards, update_shards):\n",
    "            ps.update_parameters(update_shard)\n",
    "\n",
    "\n",
    "# Test that worker_task is implemented correctly. You do not need to change this line.\n",
    "ray.get(worker_task.remote(total_dim, 1, *ps_shards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Experiment by changing the number of parameter server shards, the number of workers, and the size of the data.\n",
    "\n",
    "**NOTE:** Because these processes are all running on the same machine, network bandwidth will not be a limitation and sharding the parameter server will not help. To see the difference, you would need to run the application on multiple machines. There are still regimes where sharding a parameter server can help speed up computation on the same machine (by parallelizing the computation that the parameter server processes have to do). If you want to see this effect, you should implement a synchronous training application. In the asynchronous setting, the computation is staggered and so speeding up the parameter server usually does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "\n",
    "# Start some workers. Try changing various quantities and see how the\n",
    "# duration changes.\n",
    "start = time.time()\n",
    "ray.get([worker_task(total_dim, 5, *ps_shards) for _ in range(num_workers)])\n",
    "print('This took {} seconds.'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
