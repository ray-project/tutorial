{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "gym.logger.setLevel(gym.logging.ERROR)\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Evolution Strategies\n",
    "\n",
    "An improved approach to derivative free optimization is to maintain a *population of policies*. This is sometimes done by maintaining a set of distinct policies. In this exercise, we will maintain a *distribution over policies*. The policies will use neural nets with a fixed architecture to choose actions. **The distribution over policies will be a multivariate Gaussian over the weights of the neural nets.** The Gaussian will be represented by its mean $\\mu$. It will have a fixed covariance matrix $\\sigma I$, where $I$ is the identity matrix.\n",
    "\n",
    "The mean of the Gaussian will be initialized to the vector $\\mu_0$ of all zeros (referred to as `mean_policy` in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the mean policy to all zeros.\n",
    "num_inputs = 4\n",
    "num_hiddens = 5\n",
    "num_outputs = 1\n",
    "\n",
    "def initial_policy():\n",
    "    mean_policy = {\"weights1\": np.zeros((num_hiddens, num_inputs)),\n",
    "                   \"biases1\": np.zeros((num_hiddens)),\n",
    "                   \"weights2\": np.zeros((num_outputs, num_hiddens)),\n",
    "                   \"biases2\": np.zeros(num_outputs)}\n",
    "    return mean_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "We will use the helper function below to compute an action given a policy and an input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_action(policy, state):\n",
    "    hiddens = np.maximum(np.dot(policy[\"weights1\"], state) + policy[\"biases1\"], 0)\n",
    "    output = np.dot(policy[\"weights2\"], hiddens) + policy[\"biases2\"]\n",
    "    assert output.size == 1\n",
    "    # Turn output into a probability using a sigmoid function.\n",
    "    probability_of_0 = 1 / (1 + np.exp(-output[0]))\n",
    "    return 0 if np.random.uniform(0, 1) < probability_of_0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time $t$, we will generate an updated mean vector $\\mu_t$ as follows. We will generate $N$ samples $\\epsilon_1, \\ldots, \\epsilon_N$ from a standard multivate normal.\n",
    "\n",
    "\\begin{equation}\n",
    "\\epsilon_n \\sim \\mathcal N(0, I) \\quad \\text{for $1 \\le n \\le N$}\n",
    "\\end{equation}\n",
    "\n",
    "For each **noise vector** $\\epsilon_n$, we will derive a **perturbed policy** $\\mu_t + \\sigma\\epsilon_n$. Note that **this perturbed policy is a sample from the distribution over policies**.\n",
    "\n",
    "For variance reduction, we will generate perturbed policies in pairs, with opposite perturbations. This is shown in the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.02\n",
    "\n",
    "def generate_perturbed_policies(policy):\n",
    "    new_policy1 = dict()\n",
    "    new_policy2 = dict()\n",
    "    for key, weights in policy.items():\n",
    "        perturbation = sigma * np.random.normal(size=weights.shape)\n",
    "        new_policy1[key] = weights + perturbation\n",
    "        new_policy2[key] = weights - perturbation\n",
    "    return new_policy1, new_policy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the $N$ perturbed policies, we will simulate some rollouts and compute the average reward of these rollouts $R_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action = compute_action(policy, state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "    return cumulative_reward\n",
    "\n",
    "def evaluate_perturbed_policies(mean_policy, N):\n",
    "    policy1, policy2 = generate_perturbed_policies(mean_policy)\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #env = TestEnvironment1()\n",
    "    reward1 = rollout_policy(env, policy1)\n",
    "    reward2 = rollout_policy(env, policy2)\n",
    "    return [policy1, policy2], [reward1, reward2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then **update the mean of the distribution over policies by moving it in the direction of the perturbed policies that performed better**. That is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_{t+1} = \\mu_t + \\frac{\\alpha}{N \\sigma} \\sum_{n=1}^N R_n \\epsilon_n,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "**This is a stochastic estimate of the gradient of the expected reward of a policy generated from the random distribution taken with respect to the mean of the distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "def update_mean_policy(mean_policy, perturbed_policies, rewards):\n",
    "    new_mean_policy = copy.deepcopy(mean_policy)\n",
    "    N = len(perturbed_policies)\n",
    "    for policy, reward in zip(perturbed_policies, rewards):\n",
    "        for key in mean_policy:\n",
    "            new_mean_policy[key] += (alpha / (N * sigma)) * (policy[key] - mean_policy[key]) * reward\n",
    "    return new_mean_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we learn a policy by iteratively improving the mean of a distribution of policies. We improve the mean by sampling policies from the distribution over policies and moving the mean in the direction of the policies that performed better.\n",
    "\n",
    "In code, we have the following.\n",
    "\n",
    "**Exercise:** Parallelize the algorithm below using Ray. You may want to turn `evaluate_perturbed_policies` into a remote function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_policy = initial_policy()\n",
    "\n",
    "num_iters = 10\n",
    "for _ in range(num_iters):\n",
    "    perturbed_policies_and_rewards = [evaluate_perturbed_policies(mean_policy, 100)\n",
    "                                      for _ in range(100)]\n",
    "    all_perturbed_policies = []\n",
    "    all_rewards = []\n",
    "    for perturbed_policies, rewards in perturbed_policies_and_rewards:\n",
    "        all_perturbed_policies.extend(perturbed_policies)\n",
    "        all_rewards.extend(rewards)\n",
    "    print(\"Average reward is {}.\".format(np.mean(all_rewards)))\n",
    "    \n",
    "    mean_policy = update_mean_policy(mean_policy, all_perturbed_policies, all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about efficiency:** In the above example, we're returning the entire perturbation vectors. In the cluster setting, this may require shipping fairly large parameter vectors across the network, which can be expensive. It turns out that it's unnecessary for this algorithm. Because the perturbation vectors are generated randomly, it suffices to ship the seed that was used to generate the perturbations so that they can be regenerated on the other side. This strategy can be used to eliminate nearly all required communication for this algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
