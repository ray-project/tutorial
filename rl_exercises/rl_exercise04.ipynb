{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 4 - Asynchronous Advantage Actor Critic\n",
    "\n",
    "**GOAL:** The goal of this exercise is to demonstrate how to use the asynchronous advantage actor critic (A3C) algorithm.\n",
    "\n",
    "To understand how to use **Ray RLlib**, see the documentation at http://ray.readthedocs.io/en/latest/rllib.html.\n",
    "\n",
    "A3C is described in detail in https://arxiv.org/abs/1602.01783.\n",
    "\n",
    "In A3C, the driver maintains the most up-to-date policy. It creates a number of actors which are used to compute perform partial rollouts and to compute gradient updates to the model. The driver runs in a loop in which it waits for a single actor task to finish, updates the model with the result of the actor task, and launches a new actor task with the updated model. Because the actor tasks may run in any order, the algorithm is fundamentally asynchronous and non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.a3c import A3CAgent, DEFAULT_CONFIG\n",
    "from ray.rllib.a3c.shared_model import SharedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray. This must be done before we instantiate any RL agents. We pass in `num_workers=0` because the training agent's constructor will create a number of actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an A3CAgent object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
    "- `batch_size` is the number of simulator steps that each actor will batch together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['batch_size'] = 10\n",
    "\n",
    "agent = A3CAgent(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Train the agent for some number of steps on the CartPole environment. Compare the performance to PPO from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Implement this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Instantiate an A3CAgent object on the `MountainCar-v0` environment and train it for some number of steps. Compare the performance to PPO from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Implement this.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
