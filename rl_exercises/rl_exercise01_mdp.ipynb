{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "**The key abstraction in reinforcement learning is the Markov decision process (MDP).** An MDP models sequential interactions with an external environment. It consists of the following:\n",
    "- a **state space**\n",
    "- a set of **actions**\n",
    "- a **transition function** which describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and action $a$ was taken\n",
    "- a **reward function**, which determines the reward received at time $t$\n",
    "- a **discount factor** $\\gamma$\n",
    "More details are available [here](https://en.wikipedia.org/wiki/Markov_decision_process).\n",
    "\n",
    "**Note:** Reinforcement learning algorithms are often applied to problems that don't strictly fit into the MDP framework. In particular, situations in which the state of the environment is not fully observed lead to violations of the MDP assumption. Nevertheless, RL algorithms can be applied anyway.\n",
    "\n",
    "## Policies\n",
    "\n",
    "A **policy** is a function that takes in a **state** and returns an **action**. A policy may be stochastic (i.e., it may sample from a probability distribution) or it can be deterministic.\n",
    "\n",
    "The **goal of reinforcement learning** is to learn a **policy** for maximizing the cumulative reward in an MDP. That is, we wish to find a policy $\\pi$ which solves the following optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t,\n",
    "\\end{equation}\n",
    "\n",
    "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$) and $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$).\n",
    "\n",
    "A number of algorithms are available for solving reinforcement learning problems. Several of the most widely known are [value iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), [policy iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), and [Q learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "## RL in Python\n",
    "\n",
    "The `gym` module provides MDP interfaces to a variety of simulators. For example, the CartPole environment interfaces with a simple simulator which simulates the physics of balancing a pole on a cart. This example fits into the MDP framework as follows.\n",
    "- The **state** consists of the position and velocity of the cart as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
    "- The **actions** are to decrease or increase the cart's velocity by one unit.\n",
    "- The **transition function** is deterministic and is determined by simulating physical laws.\n",
    "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
    "- The **discount factor** in this case can be taken to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below illustrates how to create and manipulate MDPs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Create a new environment (MDP).\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Reset the state of the environment. This returns its initial state.\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `env.step` method takes an action (in the case of the CartPole environment, the appropriate actions are 0 or 1, for moving left or right). It returns a tuple of four things:\n",
    "1. the new state of the environment\n",
    "2. a reward\n",
    "3. a boolean indicating whether the simulation has finished\n",
    "4. a dictionary of miscellaneous extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Simulate taking an action in the environment. Appropriate actions for\n",
    "# the CartPole environment are 0 and 1 (for moving left and right).\n",
    "action = 0\n",
    "state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement a function `rollout`, which takes an environment, resets the state of the environment, takes random actions until the simulation has finished, and returns the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rollout(env):\n",
    "    # This function should do the following:\n",
    "    # - reset the state of the environment\n",
    "    # - take random actions until the simulation has finished\n",
    "    # - return the cumulative reward\n",
    "    raise NotImplementedError\n",
    "    \n",
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Modify the rollout function to take an environment *and* a policy. The *policy* is a function that takes in a *state* and returns an *action*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    # This function should do the following:\n",
    "    # - reset the state of the environment\n",
    "    # - take actions according to the policy until the simulation has finished\n",
    "    # - return the cumulative reward\n",
    "    raise NotImplementedError\n",
    "\n",
    "def sample_policy(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "reward = rollout_policy(env, sample_policy)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Modify the `rollout_policy` function to create a video showing the policy in action. This requires two steps:\n",
    "1. Wrap the environment as follows `env = gym.wrappers.Monitor(env, '/tmp/cartpole-experiment-1')`.\n",
    "2. Call `env.render()` whenever you want to add a frame to the video.\n",
    "\n",
    "**Note:** Producing the video may appear to freeze your computer. This works better when run from a regular Python interpreter, but you can solve the problem in the notebook by clicking on **Kernel -> Restart**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_video_of_rollout(env, policy):\n",
    "    env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1')\n",
    "    raise NotImplementedError\n",
    "\n",
    "# The video will be saved in the directory /tmp/cartpole-experiment-1.\n",
    "produce_video_of_rollout(env, sample_policy)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
