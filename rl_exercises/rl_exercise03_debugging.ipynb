{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging RL Algorithms\n",
    "\n",
    "Debugging RL algorithms can be challenging. If you implement an algorithm and run it on a challenging benchmark, you may not see it learning. This can mean a number of things.\n",
    "1. There is a bug in the algorithm.\n",
    "2. It's learning extremely slowly, so you can't tell.\n",
    "3. The algorithm is correct but it isn't learning because of the hyperparameter choices.\n",
    "\n",
    "**A quick sanity check is to run the algorithm on a very simple problem.** If you can't get it to work on the simple problem, then there is probably a bug in the algorithm. If it does work on the simple problem, then the problem may be with the hyperparameter choices or the speed at which the algorithm is learning.\n",
    "\n",
    "Below is an example test environment in which the reward is -1 if the action is 0 and the reward is 1 if the action is 1. The reward completely ignores the state of of the environment. The optimal policy is to always take action 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnvironment1(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.zeros(4)\n",
    "        self.iter = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action not in [0, 1]:\n",
    "            raise ValueError(\"The action must be either 0 or 1.\")\n",
    "        self.iter += 1\n",
    "        reward = -1 if action == 0 else 1\n",
    "        done = self.iter == 20\n",
    "        info = {}\n",
    "        return self.state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the following test environment:\n",
    "- The state is an integer initialized at 0.\n",
    "- The possible actions are 0 and 1.\n",
    "- An action of 0 decrements the state by 1. An action of 1 increments the state by 1.\n",
    "- After taking an action, the reward is 1 if the new state is greater than or equal to 5. Otherwise it is 0.\n",
    "- The environment terminates after 20 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnvironment2(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "test_env = TestEnvironment2()\n",
    "\n",
    "# TODO(rkn): Add unit tests to verify that the environment was implemented\n",
    "# correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These environments will come in handy when debugging the RL algorithms in the next exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
