{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Free Optimization\n",
    "\n",
    "The goal of reinforcement learning is to find a policy (parameterized by $\\pi$), which solves the following optimization problem.\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=0}^T R_t\n",
    "\\end{equation}\n",
    "\n",
    "Here, $R_t$ is the reward received at time $t$ when acting according to the policy $\\pi$. Note that if the environment is stochastic or the policy is stochastic, then each $R_t$ will be a random variable. Also note that $T$ will be a random variable. Both $R_t$ and $T$ depend on $\\pi$.\n",
    "\n",
    "Though the setup is similar to supervised learning in that in both settings we want to minimize or maximize some objective function, in supervised learning we often have an explicit formula for the objective function in terms of the parameters of interest, which enables us to symbolically compute the gradient of the objective function. So in supervised learning, we can often directly apply gradient descent to optimize the objective.\n",
    "\n",
    "In reinforcement learning, we often do not have an explicit formula for the reward function that we are trying to optimize, and so we can't easily compute gradients. For example, imagine an environment in which a robot walks until it falls over and the reward is the distance that the robot walked before it fell over. Computing the gradient of that reward with respect to the parameters of the robots policy is not straightforward.\n",
    "\n",
    "The difficulty of computing explicit gradients motivates the use of **derivative free optimization**. We will work through some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a policy that chooses an action using a randomly-generated two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerPolicy(object):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_units = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights1 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases1 = np.random.normal(size=num_hiddens)\n",
    "        self.weights2 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases2 = np.random.normal(size=num_outputs)\n",
    "    \n",
    "    def choose_action(state):\n",
    "        hiddens = np.max(np.dot(self.weights1, state) + self.biases1, 0)\n",
    "        output = np.dot(self.weights2, hiddens) + self.biases2\n",
    "        return 0 if output < 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using Ray, define a remote function that generates a random `TwoLayerPolicy`, performs 10 rollouts using a CartPole environment, and returns the average reward over those rollouts along with the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def evaluate_random_policy(self, num_rollouts):\n",
    "    # This function should do the following.\n",
    "    # - generate a TwoLayerPolicy\n",
    "    # - create a CartPole environment\n",
    "    # - do num_rollouts rollouts (perhaps using one of the functions\n",
    "    #   defined in a previous notebook)\n",
    "    # - return the average reward and the policy\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "average_reward, policy = ray.get(evaluate_random_policy.remote(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using the `evaluate_random_policy` remote function, evaluate 1000 randomly generated policies. Keep the best policy and make a note of its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 1000 randomly generated policies.\n",
    "raise NotImplementedError\n",
    "\n",
    "# Print the best score obtained.\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
