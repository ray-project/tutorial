{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 5 - Evolution Strategies\n",
    "\n",
    "**GOAL:** The goal of this exercise is to demonstrate how to use the evolution strategies (ES) algorithm.\n",
    "\n",
    "To understand how to use **Ray RLlib**, see the documentation at http://ray.readthedocs.io/en/latest/rllib.html.\n",
    "\n",
    "ES is described in detail in https://arxiv.org/abs/1703.03864.\n",
    "\n",
    "The ES algorithm works as follows.\n",
    "\n",
    "- It maintains a distribution over policies (which in this case is a multivariate Gaussian distribution over the weights of a neural network policy represented by the mean of the Gaussian $\\theta$).\n",
    "- The mean of the distribution is updated at each iteration, from $\\theta_0$ to $\\theta_1$ to $\\theta_2$ and so on.\n",
    "- At each iteration, a large number of policies are sampled from the distribution over policies and rollouts are performed using these **perturbed policies**.\n",
    "- The distribution over policies is updated by moving its mean in the direction of the perturbed policies that achieved higher reward.\n",
    "\n",
    "Of the algorithms explored so far, this one is the closest to the Monte Carlo algorithm implemented in one of the earlier exercises.\n",
    "\n",
    "**NOTE:** One interesting property of this algorithm is that it only cares about the rewards achieved in a given rollout. The algorithm does not need to know the states that were visited and so much less data has to be communicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.es import ESAgent, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray. This must be done before we instantiate any RL agents. We pass in num_workers=0 because the training agent's constructor will create a number of actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an ESAgent object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used.\n",
    "- `episodes_per_batch` is the minimum number of rollouts to perform at each iteration.\n",
    "- `timesteps_per_batch` is the minimum number of steps of the environment to perform at each iteration.\n",
    "- `noise_stdev` is the standard deviation of the multivariate Gaussian distribution over the neural net policy weights.\n",
    "- `stepsize` is the size of the update to the distribution over policies to take at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['episodes_per_batch'] = 100\n",
    "config['timesteps_per_batch'] = 1000\n",
    "config['noise_stdev'] = 0.02\n",
    "config['stepsize'] = 0.01\n",
    "config['eval_prob'] = 0.5\n",
    "\n",
    "agent = ESAgent(config, 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Train the agent for some number of steps on the CartPole environment. Compare the performance to PPO from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Implement this.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Instantiate an `ESAgent` object on the `MountainCar-v0` environment and train it for some number of steps. Compare the performance to PPO and A3C from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Implement this.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
