{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging RL Algorithms\n",
    "\n",
    "Debugging RL algorithms can be challenging. If you implement an algorithm and run it on a challenging benchmark, you may not see it learning. This can mean a number of things.\n",
    "1. There is a bug in the algorithm.\n",
    "2. It's learning extremely slowly, so you can't tell.\n",
    "3. The algorithm is correct but it isn't learning because of the hyperparameter choices.\n",
    "\n",
    "**A quick sanity check is to run the algorithm on a very simple problem.** If you can't get it to work on the simple problem, then there is probably a bug in the algorithm. If it does work on the simple problem, then the problem may be with the hyperparameter choices or the speed at which the algorithm is learning.\n",
    "\n",
    "Below is an example test environment in which the reward is -1 if the action is 0 and the reward is 1 if the action is 1. The reward completely ignores the state of of the environment. The optimal policy is to always take action 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnvironment1(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.zeros(4)\n",
    "        self.iter = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action not in [0, 1]:\n",
    "            raise ValueError(\"The action must be either 0 or 1.\")\n",
    "        self.iter += 1\n",
    "        reward = -1 if action == 0 else 1\n",
    "        done = self.iter == 20\n",
    "        info = {}\n",
    "        return self.state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement a policy by hand that is optimal for this test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy1(state):\n",
    "    # This policy should return an action that is optimal for this\n",
    "    # environment.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def do_rollout(env, policy):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        state, reward, done, info = env.step(policy(state))\n",
    "        cumulative_reward += reward\n",
    "    return cumulative_reward\n",
    "\n",
    "env1 = TestEnvironment1()\n",
    "# The optimal policy should achieve the maximum reward.\n",
    "assert do_rollout(env1, optimal_policy1) == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the following test environment:\n",
    "- The state is an integer initialized at 0.\n",
    "- The possible actions are 0 and 1.\n",
    "- An action of 0 decrements the state by 1. An action of 1 increments the state by 1.\n",
    "- After taking an action, the reward is 1 if the new state is greater than 5. Otherwise it is 0.\n",
    "- The environment terminates after 20 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnvironment2(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "test_env = TestEnvironment2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement a policy by hand that is optimal for this test environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal_policy2(state):\n",
    "    # This policy should return an action that is optimal for this\n",
    "    # environment.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "env1 = TestEnvironment1()\n",
    "# The optimal policy should achieve the maximum reward.\n",
    "assert do_rollout(env2, optimal_policy2) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These environments will come in handy when debugging the RL algorithms in the next exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
