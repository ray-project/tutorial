{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Evolution Strategies\n",
    "\n",
    "An improved approach to derivative free optimization is to maintain a *population of policies*. This is sometimes done by maintaining a set of distinct policies. In this exercise, we will maintain a *distribution over policies*. The policies will use neural nets with a fixed architecture to choose actions. **The distribution over policies will be a multivariate Gaussian over the weights of the neural nets.** The Gaussian will be represented by its mean $\\mu$. It will have a fixed covariance matrix (some multiple of the identity).\n",
    "\n",
    "The mean of the Gaussian will be initialized to the vector $\\mu_0$ of all zeros. At time $t$, we will generate an updated mean vector $\\mu_t$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the mean policy to all zeros.\n",
    "num_inputs = 4\n",
    "num_hiddens = 10\n",
    "num_outputs = 1\n",
    "\n",
    "def initial_policy():\n",
    "    mean_policy = {\"weights1\": np.zeros((num_hiddens, num_inputs)),\n",
    "                   \"biases1\": np.zeros((num_hiddens)),\n",
    "                   \"weights2\": np.zeros((num_outputs, num_hiddens)),\n",
    "                   \"biases2\": np.zeros(num_outputs)}\n",
    "    return mean_policy\n",
    "\n",
    "# This is a helper function for computing an action given a policy and a state.\n",
    "def compute_action(policy, state):\n",
    "    hiddens = np.maximum(np.dot(policy[\"weights1\"], state) + policy[\"biases1\"], 0)\n",
    "    output = np.dot(policy[\"weights2\"], hiddens) + policy[\"biases2\"]\n",
    "    assert output.size == 1\n",
    "    # Turn output into a probability using a sigmoid function.\n",
    "    probability_of_0 = 1 / (1 + np.exp(-output[0]))\n",
    "    return 0 if np.random.uniform(0, 1) < probability_of_0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate $N$ samples from the distribution over policies.\n",
    "\n",
    "\\begin{equation}\n",
    "\\qquad \\theta_n \\sim \\mathcal N(\\mu_t, \\sigma I) \\quad \\text{for $1 \\le n \\le N$}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal N(\\mu,\\Sigma)$ represents a multivariate Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$ and $I$ is the identity matrix. For practical reasons, we will generate perturbed policies in pairs, with opposite perturbations. This is shown in the next box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 1e-2\n",
    "\n",
    "def generate_perturbed_policies(policy):\n",
    "    new_policy1 = dict()\n",
    "    new_policy2 = dict()\n",
    "    for key, weights in policy.items():\n",
    "        perturbation = sigma * np.random.normal(size=weights.shape)\n",
    "        new_policy1[key] = weights + perturbation\n",
    "        new_policy2[key] = weights - perturbation\n",
    "    return new_policy1, new_policy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each policy $\\theta_n$, we will perform a rollout to obtain cumulative reward $R_n$. These rewards will be used to update the mean policy via the formula\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_t = \\mu_{t-1} + \\frac{\\alpha}{N \\sigma} \\sum_{n=1}^N R_n .\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\alpha$ is the learning rate.\n",
    "\n",
    "TODO(rkn): Finish the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1e-2\n",
    "\n",
    "def update_mean_policy(mean_policy, policy_perturbations, rewards):\n",
    "    batch_size = len(perturbed_policies)\n",
    "    for policy, reward in zip(perturbed_policies, rewards):\n",
    "        for key in mean_policy:\n",
    "            mean_policy[key] += (alpha / (batch_size * sigma)) * (policy[key] - mean_policy[key]) * reward\n",
    "    return mean_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Define a remote function which takes the \"mean policy\" $\\mu$, generates policies $\\mu + \\epsilon$ and $\\mu - \\epsilon$, where $\\epsilon \\sim \\mathcal N(0, \\sigma I)$ and returns the vectors $\\mu + \\epsilon$ and $\\mu - \\epsilon$ along with the rewards obtained by performing $N$ rollouts using those policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def evaluate_perturbed_policies(mean_policy, N):\n",
    "    # This function should do the following:\n",
    "    # - perturb the mean policy to generate two new policies\n",
    "    # - create a gym environment\n",
    "    # - do rollouts with the two policies (see the rollout_policy function\n",
    "    #   from the first notebook)\n",
    "    # - return the two policies and the average rewards from the rollouts\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using the `evaluate_perturbed_policies` remote function, implement the natural evolution strategies algorithm.\n",
    "\n",
    "**Note:** If it doesn't appear to be learning, try the following.\n",
    "- Debug it using the test environments created in an earlier notebook.\n",
    "- Print the magnitudes of the weights and the gradients to see if anything is too big or too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_policy = initial_policy()\n",
    "\n",
    "num_iters = 100\n",
    "for _ in range(num_iters):\n",
    "    # Run the remote function a bunch of times and get the results.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Collect the results into a big list of perturbed policies and a\n",
    "    # list of the corresponding rewards.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Update the mean_policy.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    # Print the current average reward.\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about efficiency:** In the above example, we're returning the entire perturbation vectors. In the cluster setting, this may require shipping fairly large parameter vectors across the network, which can be expensive. It turns out that it's unnecessary for this algorithm. Because the perturbation vectors are generated randomly, it suffices to ship the seed that was used to generate the perturbations so that they can be regenerated on the other side. This strategy can be used to eliminate nearly all required communication for this algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
