{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 2 - Derivative Free Optimization\n",
    "\n",
    "**GOAL:** The goal if this exercise is to show how to use Ray to implement a simple Monte Carlo algorithm for reinforcement learning.\n",
    "\n",
    "The goal of reinforcement learning is to find a policy (parameterized by $\\pi$), which solves the following optimization problem.\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=0}^T R_t(\\pi)\n",
    "\\end{equation}\n",
    "\n",
    "Here, $R_t$ is the reward received at time $t$ when acting according to the policy $\\pi$. Note that if the environment is stochastic or the policy is stochastic, then each $R_t$ will be a random variable. Also note that $T$ will be a random variable. Both $R_t$ and $T$ depend on $\\pi$.\n",
    "\n",
    "Though the setup is similar to supervised learning in that in both settings we want to minimize or maximize some objective function, in supervised learning we often have an explicit formula for the objective function in terms of the parameters of interest, which enables us to symbolically compute the gradient of the objective function. So in supervised learning, we can often directly apply gradient descent to optimize the objective.\n",
    "\n",
    "In reinforcement learning, we often do not have an explicit formula for the reward function that we are trying to optimize, and so we can't easily compute gradients. For example, imagine an environment in which a robot walks until it falls over and the reward is the distance that the robot walked before it fell over. Computing the gradient of that reward with respect to the parameters of the robot's policy is not straightforward.\n",
    "\n",
    "The difficulty of computing explicit gradients motivates the use of **derivative free optimization**. We will work through some examples below.\n",
    "\n",
    "**NOTE:** There is a huge variety of much more sophisticated RL algorithms. Here we are walking through the details of implementing a simple Monte Carlo algorithm. Subsequent exercises will show how to apply more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a policy that chooses an action using a randomly-generated two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPolicy(object):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs=1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_units = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights1 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases1 = np.random.normal(size=num_hiddens)\n",
    "        self.weights2 = np.random.normal(size=(num_outputs, num_hiddens))\n",
    "        self.biases2 = np.random.normal(size=num_outputs)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        hiddens = np.maximum(np.dot(self.weights1, state) + self.biases1, 0)\n",
    "        output = np.dot(self.weights2, hiddens) + self.biases2\n",
    "        assert output.size == 1\n",
    "        return 0 if output[0] < 0 else 1\n",
    "\n",
    "policy = TwoLayerPolicy(4, 5)\n",
    "# You can get an action by applying the policy to a state.\n",
    "action = policy(np.random.normal(size=4))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remote function `evaluate_random_policy` defined below generates a random `TwoLayerPolicy`, performs some rollouts using a CartPole environment, and returns the average reward over those rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You may find the helper function 'rollout_policy' helpful.\n",
    "# This implementation here is the solution to one of the exercises\n",
    "# from the previous notebook.\n",
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose an action.\n",
    "        action = policy(state)\n",
    "        # Take an action.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "@ray.remote\n",
    "def evaluate_random_policy():\n",
    "    # Generate a random policy.\n",
    "    policy = TwoLayerPolicy(4, 5)\n",
    "    \n",
    "    # Create an environment.\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    # We evaluate the same policy 100 times and then take the average in\n",
    "    # order to evaluate the policy more accurately (the environment is\n",
    "    # stochastic).\n",
    "    num_rollouts = 100\n",
    "    return np.mean([rollout_policy(env, policy) for _ in range(num_rollouts)])\n",
    "    \n",
    "average_reward = ray.get(evaluate_random_policy.remote())\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Using the `evaluate_random_policy` remote function, evaluate 100 randomly generated policies (that is, simply call `evaluate_random_policy` 100 times). Make a note of the best score.\n",
    "\n",
    "**EXERCISE:** Try 1000 instead of 100.\n",
    "\n",
    "**NOTE:** The best possible score should be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 100 randomly generated policies.\n",
    "raise NotImplementedError\n",
    "\n",
    "# Print the best score obtained. You can compute the best score using np.max.\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
