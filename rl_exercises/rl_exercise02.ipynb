{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 2 - Derivative Free Optimization\n",
    "\n",
    "**GOAL:** The goal if this exercise is to show how to use Ray to implement a simple Monte Carlo algorithm for reinforcement learning.\n",
    "\n",
    "The goal of reinforcement learning is to find a policy (parameterized by $\\pi$), which solves the following optimization problem.\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=0}^T R_t(\\pi)\n",
    "\\end{equation}\n",
    "\n",
    "Here, $R_t$ is the reward received at time $t$ when acting according to the policy $\\pi$. Note that if the environment is stochastic or the policy is stochastic, then each $R_t$ will be a random variable. Also note that $T$ will be a random variable. Both $R_t$ and $T$ depend on $\\pi$.\n",
    "\n",
    "Though the setup is similar to supervised learning in that in both settings we want to minimize or maximize some objective function, in supervised learning we often have an explicit formula for the objective function in terms of the parameters of interest, which enables us to symbolically compute the gradient of the objective function. So in supervised learning, we can often directly apply gradient descent to optimize the objective.\n",
    "\n",
    "In reinforcement learning, we often do not have an explicit formula for the reward function that we are trying to optimize, and so we can't easily compute gradients. For example, imagine an environment in which a robot walks until it falls over and the reward is the distance that the robot walked before it fell over. Computing the gradient of that reward with respect to the parameters of the robot's policy is not straightforward.\n",
    "\n",
    "The difficulty of computing explicit gradients motivates the use of **derivative free optimization**. We will work through some examples below.\n",
    "\n",
    "**NOTE:** There is a huge variety of much more sophisticated RL algorithms. Here we are walking through the details of implementing a simple Monte Carlo algorithm. Subsequent exercises will show how to apply more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a policy that chooses an action using a randomly-generated two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPolicy(object):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs=1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_units = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights1 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases1 = np.random.normal(size=num_hiddens)\n",
    "        self.weights2 = np.random.normal(size=(num_outputs, num_hiddens))\n",
    "        self.biases2 = np.random.normal(size=num_outputs)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        hiddens = np.maximum(np.dot(self.weights1, state) + self.biases1, 0)\n",
    "        output = np.dot(self.weights2, hiddens) + self.biases2\n",
    "        assert output.size == 1\n",
    "        return 0 if output[0] < 0 else 1\n",
    "\n",
    "policy = TwoLayerPolicy(4, 5)\n",
    "# You can get an action by applying the policy to a state.\n",
    "action = policy(np.random.normal(size=4))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Using Ray, define a remote function that generates a random `TwoLayerPolicy`, performs 10 rollouts using a CartPole environment, and returns the average reward over those rollouts along with the policy.\n",
    "\n",
    "**NOTE:** The `rollout_policy` helper function provided below may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You may find the helper function 'rollout_policy' helpful.\n",
    "# This implementation here is the solution to one of the exercises\n",
    "# from the previous notebook.\n",
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose an action.\n",
    "        action = policy(state)\n",
    "        # Take an action.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "@ray.remote\n",
    "def evaluate_random_policy(num_rollouts):\n",
    "    # Generate a random policy.\n",
    "    policy = TwoLayerPolicy(4, 5)\n",
    "    \n",
    "    # Create an environment.\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    # EXERCISE: Do 'num_rollouts' rollouts using the policy and the\n",
    "    # environment, and return the average reward obtained by the rollouts.\n",
    "    raise NotImplementedError\n",
    "    \n",
    "average_reward = ray.get(evaluate_random_policy.remote(10))\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Using the `evaluate_random_policy` remote function, evaluate 100 randomly generated policies. Make a note of the best score. Try taking the best of 1000.\n",
    "\n",
    "**NOTE:** The best possible score should be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate 100 randomly generated policies.\n",
    "raise NotImplementedError\n",
    "\n",
    "# Print the best score obtained.\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
