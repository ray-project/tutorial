{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Free Optimization\n",
    "\n",
    "The goal of reinforcement learning is to find a policy (parameterized by $\\pi$), which solves the following optimization problem.\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\pi} \\sum_{t=0}^T R_t\n",
    "\\end{equation}\n",
    "\n",
    "Here, $R_t$ is the reward received at time $t$ when acting according to the policy $\\pi$. Note that if the environment is stochastic or the policy is stochastic, then each $R_t$ will be a random variable. Also note that $T$ will be a random variable. Both $R_t$ and $T$ depend on $\\pi$.\n",
    "\n",
    "Though the setup is similar to supervised learning in that in both settings we want to minimize or maximize some objective function, in supervised learning we often have an explicit formula for the objective function in terms of the parameters of interest, which enables us to symbolically compute the gradient of the objective function. So in supervised learning, we can often directly apply gradient descent to optimize the objective.\n",
    "\n",
    "In reinforcement learning, we often do not have an explicit formula for the reward function that we are trying to optimize, and so we can't easily compute gradients. For example, imagine an environment in which a robot walks until it falls over and the reward is the distance that the robot walked before it fell over. Computing the gradient of that reward with respect to the parameters of the robots policy is not straightforward.\n",
    "\n",
    "The difficulty of computing explicit gradients motivates the use of **derivative free optimization**. We will work through some examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a policy that chooses an action using a randomly-generated two-layer neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPolicy(object):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs=1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_units = num_hiddens\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights1 = np.random.normal(size=(num_hiddens, num_inputs))\n",
    "        self.biases1 = np.random.normal(size=num_hiddens)\n",
    "        self.weights2 = np.random.normal(size=(num_outputs, num_hiddens))\n",
    "        self.biases2 = np.random.normal(size=num_outputs)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        hiddens = np.maximum(np.dot(self.weights1, state) + self.biases1, 0)\n",
    "        output = np.dot(self.weights2, hiddens) + self.biases2\n",
    "        assert output.size == 1\n",
    "        return 0 if output[0] < 0 else 1\n",
    "\n",
    "policy = TwoLayerPolicy(4, 5)\n",
    "# You can get an action by applying the policy to a state.\n",
    "action = policy(np.random.normal(size=4))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using Ray, define a remote function that generates a random `TwoLayerPolicy`, performs 10 rollouts using a CartPole environment, and returns the average reward over those rollouts along with the policy.\n",
    "\n",
    "**Note:** You may want to copy over the function `rollout_policy` from an earlier notebook to use as a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def evaluate_random_policy(num_rollouts):\n",
    "    # This function should do the following.\n",
    "    # - generate a TwoLayerPolicy\n",
    "    # - create a CartPole environment with gym.make('CartPole-v0')\n",
    "    # - do num_rollouts rollouts (perhaps using one of the functions\n",
    "    #   defined in a previous notebook)\n",
    "    # - return the average reward and the policy\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "policy, average_reward = ray.get(evaluate_random_policy.remote(10))\n",
    "print(policy)\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using the `evaluate_random_policy` remote function, evaluate 100 randomly generated policies. Keep the best policy and make a note of its score. Try taking the best of 1000.\n",
    "\n",
    "**Note:** The best possible score should be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate 100 randomly generated policies.\n",
    "raise NotImplementedError\n",
    "\n",
    "# Print the best score obtained.\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
