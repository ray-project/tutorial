{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CartPole with Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will cover how to parallelize a policy gradient algorithm. **Try running the example below.** It is a serial implementation of a policy gradient algorithm for learning a policy in the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "\n",
    "n_obs = 4              # dimensionality of observations\n",
    "n_h = 256              # number of hidden layer neurons\n",
    "n_actions = 2          # number of available actions\n",
    "learning_rate = 5e-4   # how rapidly to update parameters\n",
    "gamma = .99            # reward discount factor\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "observations, rewards, labels = [], [], []\n",
    "reward_sum = 0\n",
    "reward_sums = []\n",
    "episode_number = 0\n",
    "num_timesteps = 0\n",
    "\n",
    "def make_policy(observation_placeholder):\n",
    "    hidden = slim.fully_connected(observation_placeholder, n_h)\n",
    "    log_probability = slim.fully_connected(hidden, n_actions, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(0.001))\n",
    "    return tf.nn.softmax(log_probability)\n",
    "\n",
    "def discounted_normalized_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute normalized discounted reward.\"\"\"\n",
    "    result = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_sum = running_sum * gamma + r[t]\n",
    "        result[t] = running_sum\n",
    "    return (result - np.mean(result)) / np.std(result)\n",
    "        \n",
    "input_observation = tf.placeholder(dtype=tf.float32, shape=[None, n_obs])\n",
    "input_probability = tf.placeholder(dtype=tf.float32, shape=[None, n_actions])\n",
    "input_reward = tf.placeholder(dtype=tf.float32, shape=[None,1])\n",
    "\n",
    "# The policy network.\n",
    "action_probability = make_policy(input_observation)\n",
    "\n",
    "loss = tf.nn.l2_loss(input_probability - action_probability)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, grad_loss=input_reward)\n",
    "\n",
    "# Create TensorFlow session and initialize variables.\n",
    "sess = tf.InteractiveSession()\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "num_timesteps = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for _ in range(100000):\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    probability = sess.run(action_probability, {input_observation: observation[np.newaxis, :]})[0,:]\n",
    "\n",
    "    action = np.random.choice(n_actions, p = probability)\n",
    "    label = np.zeros_like(probability) ; label[action] = 1\n",
    "    observations.append(observation)\n",
    "    labels.append(label)\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if done:\n",
    "        timesteps = len(rewards)\n",
    "\n",
    "        if (num_timesteps + timesteps) // 5000 > num_timesteps // 5000:\n",
    "            print('time: {:4.1f}, timesteps: {:7.0f}, reward: {:7.3f}'.format(\n",
    "                time.time() - start_time, num_timesteps + timesteps, np.mean(reward_sums)))\n",
    "\n",
    "        num_timesteps += timesteps\n",
    "\n",
    "        feed = {input_observation: np.vstack(observations),\n",
    "                input_reward: discounted_normalized_rewards(np.vstack(rewards)),\n",
    "                input_probability: np.vstack(labels)}\n",
    "        sess.run(train_op, feed)\n",
    "        observations, rewards, labels = [], [], [] # Reset history.\n",
    "\n",
    "        episode_number += 1\n",
    "        observation = env.reset()\n",
    "        reward_sums.append(reward_sum)\n",
    "        reward_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Follow the instructions in the code below to parallelize the policy gradient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this Exercise we will parallelize the example cartpole.py\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "\n",
    "n_obs = 4              # dimensionality of observations\n",
    "n_h = 256              # number of hidden layer neurons\n",
    "n_actions = 2          # number of available actions\n",
    "learning_rate = 5e-4   # how rapidly to update parameters\n",
    "gamma = .99            # reward discount factor\n",
    "\n",
    "ray.init()\n",
    "\n",
    "def make_policy(observation_placeholder):\n",
    "    hidden = slim.fully_connected(observation_placeholder, n_h)\n",
    "    log_probability = slim.fully_connected(hidden, n_actions, activation_fn=None, weights_initializer=tf.truncated_normal_initializer(0.001))\n",
    "    return tf.nn.softmax(log_probability)\n",
    "\n",
    "def discounted_normalized_rewards(r):\n",
    "    \"\"\"Take 1D float array of rewards and compute normalized discounted reward.\"\"\"\n",
    "    result = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_sum = running_sum * gamma + r[t]\n",
    "        result[t] = running_sum\n",
    "    return (result - np.mean(result)) / np.std(result)\n",
    "\n",
    "# EXERCISE: Make the Agent class an actor and fill in the __init__\n",
    "# as well as the rollout methods.\n",
    "class Agent(object):\n",
    "\n",
    "    # EXERCISE: Create the CartPole-v1 environment as self.env,\n",
    "    # create placeholders for the input observation, set up the action_probability\n",
    "    # network, create a tensorflow Session, initialize the variables and\n",
    "    # create a TensorFlowVariables objects that can be used to read and\n",
    "    # write the variables of the policy.\n",
    "    def __init__(self):\n",
    "        #\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        self.input_observation = tf.placeholder(dtype=tf.float32, shape=[None, n_obs])\n",
    "\n",
    "        # Set up the policy network.\n",
    "        self.action_probability = make_policy(self.input_observation)\n",
    "\n",
    "        # Create TensorFlow session and initialize variables.\n",
    "        self.sess = tf.Session()\n",
    "        tf.initialize_all_variables().run(session=self.sess)\n",
    "\n",
    "        self.variables = ray.experimental.TensorFlowVariables(self.action_probability, self.sess)\n",
    "\n",
    "    # EXERCISE: Write the function to load the weights into the policy here.\n",
    "    def load_weights(self, weights):\n",
    "        self.variables.set_weights(weights)\n",
    "\n",
    "    def rollout(self):\n",
    "        done = False\n",
    "        observations, rewards, labels = [], [], []\n",
    "        observation = self.env.reset()\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # EXERCISE:\n",
    "            # Write the part that evaluates the policy and appends\n",
    "            # the current observation to observations, the reward to rewards and\n",
    "            # the target label to labels.\n",
    "\n",
    "        return np.vstack(observations), discounted_normalized_rewards(np.vstack(rewards)), np.vstack(labels)\n",
    "\n",
    "\n",
    "\n",
    "agents = # EXERCISE: Create 4 remote Agents here\n",
    "\n",
    "input_observation = tf.placeholder(dtype=tf.float32, shape=[None, n_obs])\n",
    "input_probability = tf.placeholder(dtype=tf.float32, shape=[None, n_actions])\n",
    "input_reward = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "action_probability = make_policy(input_observation)\n",
    "\n",
    "loss = tf.nn.l2_loss(input_probability - action_probability)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, grad_loss=input_reward)\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.initialize_all_variables().run(session=sess)\n",
    "variables = ray.experimental.TensorFlowVariables(loss, sess)\n",
    "\n",
    "num_timesteps = 0\n",
    "reward_sums = []\n",
    "\n",
    "# Barrier for the timing (TODO(pcm): clean this up)\n",
    "weights = ray.put(variables.get_weights())\n",
    "ray.get([agent.load_weights.remote(weights) for agent in agents])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(100):\n",
    "    weights = ray.put(variables.get_weights())\n",
    "    # EXERCISE: Set weights on the remote agents\n",
    "    # EXERCISE: Call agent.rollout on all the agents, get results, store them in variable \"trajectories\"\n",
    "    reward_sums.extend([trajectory[0].shape[0] for trajectory in trajectories])\n",
    "    timesteps = np.sum([trajectory[0].shape[0] for trajectory in trajectories])\n",
    "    if (num_timesteps + timesteps) // 5000 > num_timesteps // 5000:\n",
    "        print('time: {:4.1f}, timesteps: {:7.0f}, reward: {:7.3f}'.format(\n",
    "            time.time() - start_time, num_timesteps + timesteps, np.mean(reward_sums)))\n",
    "    num_timesteps += timesteps\n",
    "    results = [np.concatenate(x) for x in zip(*trajectories)]\n",
    "    sess.run(train_op, {input_observation: results[0], input_reward: results[1], input_probability: results[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
